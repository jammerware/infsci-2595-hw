---
title: "Part ivB: Interpretation (Difficult customers)"
subtitle: "INFSCI 2595 Final"
author: "Ben Stein"
date: "4/27/2022"
output: html_document
knit: (function(inputFile, encoding) {
    rmarkdown::render(inputFile, encoding = encoding, output_dir = "knit") }
  )
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# About this document

In this document, I identify which customers are most difficult to predict in both regression and classification based on all resampling performed during model training.

# Setup

## Libraries & tools

```{r}
library(tidyverse)
library(caret)
library(yardstick)

source("./tools.R")
```

## Data

```{r}
df <- load_project_data(outcome_to_numeric = FALSE)
```

## "Best" models

```{r}
best_reg_model <- load_model("adv_reg_model10_svm_rbf.rds")
best_cls_model <- load_model("adv_class_model4_continuous_linear.rds")
```

# Tough customers: regression

As I noted in [ivA](./ivA_interpretation_intro.html), my top-performing regression model was an SVM with a radial basis function. Let's compute the residuals for each observation (across all resamplings) and group them by customer:

```{r}
df %>%
  select(customer) %>%
  mutate(row_index = 1:nrow(.)) %>%
  inner_join(best_reg_model$pred, by = c("row_index" = "rowIndex")) %>%
  select(
    customer,
    pred,
    obs
  ) %>%
  group_by(customer) %>%
  summarise(
    rmse = caret::RMSE(pred, obs)
  ) %>%
  arrange(rmse) %>%
  ggplot(mapping = aes(x = factor(customer, levels = customer), y = rmse)) +
  geom_col() +
  ggtitle("Top regression model: per-customer RMSE") +
  theme_bw()
```

Based on the RMSE observed across resamplings, customers K and Q are the most difficult to predict in regression. Maybe they're moody people?

# Tough customers: classification

Here, I look at how _accurately_ the best classification model predicts events for each customer:

```{r}
df %>%
  select(customer) %>%
  mutate(row_index = 1:nrow(.)) %>%
  inner_join(best_cls_model$pred, by = c("row_index" = "rowIndex")) %>%
  select(
    customer,
    pred,
    obs,
    event,
    non_event
  ) %>%
  group_by(customer) %>%
  summarise(
    acc = sum(ifelse(obs == pred, 1, 0)) / n()
  ) %>%
  arrange(desc(acc)) %>%
  ggplot(mapping = aes(x = factor(customer, levels = customer), y = acc)) +
  geom_col() +
  ggtitle("Top classification model: per-customer accuracy") + 
  theme_bw()
```

Based on this figure, accuracy is closer among customers, but we see that customers `A` and `Other` are the most difficult to classify. `Other` doesn't surprise me, since I assume it's an aggregation of a number of customers whose collective behavior may not have clear patterns. However, based on the base rate of the `event` outcome, no model has great accuracy (though we seem to be able to predict customer `E` successfully).

## How are we misclassifying problematic customers?

We know that accuracy is low for customers `Other` and `A`, but I was curious exactly how we were misclassifying them. I used a confusion matrix to check.

### Customer `Other`

```{r}
df_customerOther_preds <- df %>%
  select(customer) %>%
  mutate(row_index = 1:nrow(.)) %>%
  inner_join(best_cls_model$pred, by = c("row_index" = "rowIndex")) %>%
  select(
    customer,
    pred,
    obs,
    event,
    non_event
  ) %>%
  filter(customer == "Other") %>%
  select(
    pred,
    obs
  )

autoplot(
  yardstick::conf_mat(df_customerOther_preds, truth = obs, estimate = pred), 
  type = "heatmap"
) + ggtitle("Customer 'other': confusion matrix")
```

For this customer (which I assume is actually a group of customers), we misclassified primarily with false negatives (at a rate of nearly 3 times more false negatives than false positives). Given that the event of interest is a product _not_ meeting its sales goal, this may mean that our model is a little overoptimistic, at least in the case of this customer.

### Customer `A`

```{r}
df_customerA_preds <- df %>%
  select(customer) %>%
  mutate(row_index = 1:nrow(.)) %>%
  inner_join(best_cls_model$pred, by = c("row_index" = "rowIndex")) %>%
  select(
    customer,
    pred,
    obs,
    event,
    non_event
  ) %>%
  filter(customer == "A") %>%
  select(
    pred,
    obs
  )

autoplot(
  yardstick::conf_mat(df_customerA_preds, truth = obs, estimate = pred), 
  type = "heatmap"
) + ggtitle("Customer 'A': confusion matrix")
```

We see a similar ratio with this customer, lending credence to the idea that our model may be predicting fewer events than it should.