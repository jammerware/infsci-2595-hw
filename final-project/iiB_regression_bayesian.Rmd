---
title: "Part iiB: Bayesian Regression Models"
subtitle: "INFSCI 2595 Final"
author: "Ben Stein"
date: "4/27/2022"
output: html_document
knit: (function(inputFile, encoding) {
    rmarkdown::render(inputFile, encoding = encoding, output_dir = "knit") }
  )
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# About this document

In this document, I evaluate two Bayesian regression models based on findings from the simple frequentist modeling done in the previous section.

**NOTE**: This document makes extensive use of code published on the course Canvas for using `rstanarm` and comparing Bayesian models.

# Setup

## Libraries and tools

```{r}
library(tidyverse)
library(rstanarm)

# set rstanarm options
options(mc.cores = parallel::detectCores())

source("./tools.R")
```

## Data

```{r}
df <- load_project_data() %>%
  select(-outcome_numeric)
```


# Models

**NOTE:** When I tried to run my best-performing model from the previous section (model 5) using `rstanarm`, I got a strange error when attempting to compute `bayes_R2`. I'll demonstrate it here, but I'll do Bayesian versions of the next-two-best models for this section.

```{r, error = TRUE}
lm_model5 <- stan_lm(
  response_log ~ customer * (.), 
  data = df %>% select(where(is.numeric), customer),
  prior = R2(location = .5),
  seed = 2022
)

bayes_R2(lm_model5)
```


## First Bayesian model (model 6 from the previous section)

Model 6 from the previous section was my second best-performing model from the previous section:

```{r}
lm_model6 <- lm(
  response_log ~ .^2, 
  data = df %>% select(where(is.numeric))
)

broom::glance(lm_model6)
```

Let's examine this model from a Bayesian perspective.

### Fit

```{r}
bayesian_model6 <- stan_lm(
  response_log ~ .^2, 
  data = df %>% select(where(is.numeric)),
  prior = R2(location = .5), # starting from a weak R^2
  seed = 2022
)
```

### Summary

```{r}
bayesian_model6 %>% summary
```

## Second Bayesian Model (Model 4 from the previous section)

I used Model 4 as my second model here because it was the third-best performing model previously and because it interacts `region` with the continuous inputs. EDA and frequentist modeling showed that `region` is likely a predictive factor, so it seems like a good guess.

### Frequentist version of this model

```{r}
lm_model4 <- lm(
  response_log ~ region * .,
  data = df %>% select(-customer)
)

broom::glance(lm_model4)
```


### Fit

```{r}
bayesian_model4 <- stan_lm(
  response_log ~ region * ., 
  data = df %>%
    select(-customer),
  prior = R2(location = .5), # starting from a weak R^2
  seed = 2022
)
```

### Summary

```{r}
bayesian_model4 %>% summary
```

# Analysis

## Comparing the two models

Let's first look at the R^2 statistic for each model.

```{r}
purrr::map2_dfr(
    list(bayesian_model6, bayesian_model4),
    as.character(c("6", "4")),
    function(mod, mod_name){
      tibble::tibble(rsquared = bayes_R2(mod)) %>% 
        mutate(model_name = mod_name)
    }
  ) %>% 
  ggplot(mapping = aes(x = rsquared)) +
  geom_freqpoly(
    bins = 55,
    mapping = aes(color = model_name),
    size = 1.1) +
  coord_cartesian(xlim = c(0, 1)) +
  ggthemes::scale_color_colorblind("Model") +
  theme_bw()
```

Under the current formulation, Model 4 seems to have the best posterior R^2.

## Posterior noise

```{r}
purrr::map2_dfr(
    list(bayesian_model6, bayesian_model4),
    as.character(c("6", "4")),
    function(mod, mod_name){
      as.data.frame(mod) %>%
        tibble::as_tibble() %>%
        select(sigma) %>% 
        mutate(model_name = mod_name)
    }
  ) %>% 
  ggplot(mapping = aes(x = sigma)) +
    geom_freqpoly(
      bins = 55, 
      mapping = aes(color = model_name), 
      size = 1.1) +
  coord_cartesian(xlim = c(0, 1)) +
  ggthemes::scale_color_colorblind("Model") +
  theme_bw()
```

Similarly, the posterior noise on Model 4 is slightly lower. Of our Bayesian models, 4 is looking pretty good.

## WAIC

We can examine the WAIC to get another perspective on how these models relate to each other. Let's calculate it and attach it to the model objects, then compare them:

```{r}
bayesian_model6$waic <- waic(bayesian_model6)
bayesian_model4$waic <- waic(bayesian_model4)

all_models <- stanreg_list(
  bayesian_model6, bayesian_model4,
  model_names = c(
    "All sentiment features (pairwise interactions)", 
    "Interacting region with continuous inputs"
  )
)

loo_compare(all_models, criterion = "waic")
```

Okay, I'll try to follow the warning messages.

## LOOIC

I did a little bit of looking at LOOIC, and it sounds like a Bayesian equivalent of other IC-style metrics. Which means that I don't exactly know how it works, but let's look at it.

```{r}
loo_6 <- loo(bayesian_model6)
loo_4 <- loo(bayesian_model4)
loo_compare(loo_6, loo_4)
```

My understanding is that the significant difference here tells us that model 6 is "better" than 4 based on LOOIC, but the warning messages make me nervous, so I bailed here.

## Conclusion

Because model 4 (the second model evaluated here) has slightly higher R^2, slightly lower posterior noise, and at least roughly equivalent WAIC, I choose it as the winner here.

# Analysis of the winning model

## Coefficient distributions

Let's start by visualizing the coefficients of the model:

```{r}
visualize_bayesian_coefficients(bayesian_model4)
```
We can also look at the actual posterior intervals for each feature, especially for features that are significant at 95%.

```{r}
get_significant_bayesian_coefficients(bayesian_model4)
```

An interesting thing to note here is that `xb_07` is one of the predictors we saw doing big work in the previous section, so it's nice that it shows up here. It seems like a good predictor. `region` also shows up in both places, suggesting that it's a good predictor.

## Quantiles for R^2 and noise

```{r}
rstanarm::bayes_R2(bayesian_model4) %>% quantile(c(.05, .5, .95))
```
```{r}
bayesian_model4 %>%
  as_tibble() %>%
  pull(sigma) %>%
  quantile(c(.05, .5, .95))
```

## Comparison to the MLE estimate from the frequentist model

We can visualize the MLE estimate from the frequentist model with the distribution of posterior sigma samples to see how they compare:

```{r}
bayesian_model4 %>%
  as.data.frame %>%
  as_tibble %>%
  ggplot(mapping = aes(x = sigma)) + 
  geom_histogram(bins = 60) + 
  geom_vline(xintercept = sigma(lm_model4$finalModel), color = "lightblue", linetype = "dashed", size = 2) +
  geom_vline(mapping = aes(xintercept = median(sigma)), color = "gold", linetype="dashed", size = 2)
```

As the figure shows, MLE on the noise from the standard `lm` model is within the margin of error for the posterior distribution of sigma in the Bayesian model. The range of the noise is also not huge (based on the scale of the x-axis), so we can be reasonably confident that the noise is around the MLE provided by the frequentist model.

## Precision on the posterior $\sigma$: How certain are we?

```{r}
bayesian_model4 %>%
  as.data.frame %>%
  as_tibble %>%
  pull(sigma) %>%
  sd
```

The standard deviation of the posterior samples on sigma is very low, suggesting that we're pretty sure that our estimation of the amount of noise is accurate.