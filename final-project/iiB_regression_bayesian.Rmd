---
title: "INFSCI 2595 Final"
subtitle: "Part iiB: Bayesian Regression Models"
author: "Ben Stein"
date: "4/27/2022"
output: html_document
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "knit") }
  )
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# About this document

In this document, I evaluate two Bayesian regression models based on findings from the simple frequentist modeling done in the previous section.

**NOTE**: This document makes extensive use of code published on the course Canvas for using `rstanarm` and comparing Bayesian models.

# Setup

## Libraries and tools

```{r}
library(tidyverse)
library(rstanarm)

# set rstanarm options
options(mc.cores = parallel::detectCores())

source("./tools.R")
```

## Data

```{r}
df <- load_project_data() %>%
  select(-outcome_numeric)
```


# Models

## First Bayesian model (model 3 from the previous section)

Model 3 from the previous section was my best-performing model (RMSE = 0.3299896, R^2 = 0.6233415). It uses all linear predictors. Let's examine this model from a Bayesian perspective.

### Fit

```{r}
bayesian_model3 <- stan_lm(
  response_log ~ ., 
  data = df,
  prior = R2(location = .5), # starting from a weak R^2
  seed = 2022
)
```

### Summary

```{r}
bayesian_model3 %>% summary
```

## Second Bayesian Model (Model 4 from the previous section)

I used Model 4 as my second model here because it was the second-best performing model previously and because it interacts `region` with the continuous inputs. Examination of the previous Bayesian model shows that `region` is likely a predictive factor, so it seems like a good guess.

### Fit

```{r}
bayesian_model4 <- stan_lm(
  response_log ~ region * ., 
  data = df %>%
    select(-customer),
  prior = R2(location = .5), # starting from a weak R^2
  seed = 2022
)
```

### Summary

```{r}
bayesian_model4 %>% summary
```

# Analysis

## Comparing the two models

Let's first look at the R^2 statistic for each model.

```{r}
purrr::map2_dfr(
    list(bayesian_model3, bayesian_model4),
    as.character(3:4),
    function(mod, mod_name){
      tibble::tibble(rsquared = bayes_R2(mod)) %>% 
      mutate(model_name = mod_name)
    }
  ) %>% 
  ggplot(mapping = aes(x = rsquared)) +
    geom_freqpoly(
      bins = 55, 
      mapping = aes(color = model_name), 
      size = 1.1) +
  coord_cartesian(xlim = c(0, 1)) +
  ggthemes::scale_color_colorblind("Model") +
  theme_bw()
```

The models are very close. Under the current formulation, Model 4 seems to have the best posterior R^2.

## Posterior noise

```{r}
purrr::map2_dfr(
    list(bayesian_model3, bayesian_model4),
    as.character(3:4),
    function(mod, mod_name){
      as.data.frame(mod) %>%
        tibble::as_tibble() %>%
        select(sigma) %>% 
        mutate(model_name = mod_name)
    }
  ) %>% 
  ggplot(mapping = aes(x = sigma)) +
    geom_freqpoly(
      bins = 55, 
      mapping = aes(color = model_name), 
      size = 1.1) +
  coord_cartesian(xlim = c(0, 1)) +
  ggthemes::scale_color_colorblind("Model") +
  theme_bw()
```

Similarly, the posterior noise on Model 4 is slightly lower.

## WAIC

We can examine the WAIC to get another perspective on how these models relate to each other. Let's calculate it and attach it to the model objects, then compare them:

```{r}
bayesian_model3$waic <- waic(bayesian_model3)
bayesian_model4$waic <- waic(bayesian_model4)

all_models <- stanreg_list(
  bayesian_model3, bayesian_model4,
  model_names = c(
    "All features (linear additive)", 
    "Interacting region with continuous inputs"
  )
)

loo_compare(all_models, criterion = "waic")
```

This is inconclusive since the actual difference between the models is less than the standard error.

I start by looking at the posterior intervals on the variables to see which ones seem significant.

## Conclusion

Because model 4 (the second model evaluated here) has slightly higher R^2, slightly lower posterior noise, and at least roughly equivalent WAIC, I choose it as the winner here.

# Analysis of the winning model

## Coefficient distributions

Let's start by visualizing the coefficients of the model:

```{r}
visualize_bayesian_coefficients(bayesian_model4)
```
We can also look at the actual posterior intervals for each feature, especially for features that are significant at 95%.

```{r}
get_significant_bayesian_coefficients(bayesian_model4)
```

## Quantiles for R^2 and noise

```{r}
rstanarm::bayes_R2(bayesian_model3) %>% quantile(c(.05, .5, .95))
```
```{r}
bayesian_model4 %>%
  as_tibble() %>%
  pull(sigma) %>%
  quantile(c(.05, .5, .95))
```

## Comparison to the MLE estimate from the frequentist model

We can visualize the MLE estimate from the frequentist model with the distribution of posterior sigma samples to see how they compare:

```{r}
bayesian_model4 %>%
  as.data.frame %>%
  as_tibble %>%
  ggplot(mapping = aes(x = sigma)) + 
  geom_histogram(bins = 60) + 
  geom_vline(xintercept = sigma(frequentist_model4$finalModel), color = "lightblue", linetype = "dashed", size = 2) +
  geom_vline(mapping = aes(xintercept = median(sigma)), color = "gold", linetype="dashed", size = 2)
```

As the figure shows, MLE on the noise from the standard `lm` model is within the margin of error for the posterior distribution of sigma in the Bayesian model.

## Precision on the posterior $\sigma$: How certain are we?

```{r}
bayesian_model4 %>%
  as.data.frame %>%
  as_tibble %>%
  pull(sigma) %>%
  sd
```

The standard deviation of the posterior samples on sigma is very low, suggesting that we're pretty sure that our estimation of the amount of noise is accurate.