---
title: "INFSCI 2595 Final"
subtitle: "Part IIIa: Classification (simple models"
author: "Ben Stein"
date: "3/23/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# About this document

In this document, I fit simple classification models to predict the `outcome` variable.

# Setup

## Libraries & tools

```{r}
library(tidyverse)
library(recipes)
library(caret)

source("./tools.R")
```

## Data

```{r}
df_raw <- load_project_data(outcome_to_numeric = FALSE) %>%
  select(-response_log)

df_raw %>% glimpse
```

# Modeling

A quick utility function so I can encapsulate model training:

```{r}
train_glm <- function(formula = outcome ~ ., data) {
  set.seed(2022)
  
  model <- train(
    formula,
    data = data,
    method = "glm",
    family = "binomial",
    metric = "ROC",
    preProcess = c("center", "scale"),
    trControl = trainControl(
      method = "repeatedcv", 
      repeats = 5, 
      number = 5,
      classProbs = TRUE,
      savePredictions = TRUE,
      summaryFunction = twoClassSummary
    )
  )
  
  model
}
```


## Model 1: Categorical variables (linear additive)

```{r}
glm_model1 <- train_glm(data = df_raw %>% select(where(is.factor)))
```

## Model 2: Continuous variables (linear additive)

```{r}
glm_model2 <- train_glm(data = df_raw %>% select(where(is.numeric), outcome))
```

## Model 3: All variables (linear additive)

```{r}
glm_model3 <- train_glm(data = df_raw)
```

## Model 4: Interact region with continuous inputs

```{r}
glm_model4 <- train_glm(
  formula = outcome ~ region * .,
  data = df_raw %>% select(
    where(is.numeric),
    region,
    outcome
  )
)
```

## Model 5: Interact customer with continuous inputs

```{r}
glm_model5 <- train_glm(
  formula = outcome ~ customer * .,
  data = df_raw %>% 
    select(
      where(is.numeric),
      customer,
      outcome
    )
)
```

**TODO: this is bad**

## Model 6: All pairwise interactions of continuous inputs

```{r}
glm_model6 <- train_glm(
  formula = outcome ~ .^2,
  data = df_raw %>% select(where(is.numeric), outcome)
)
```

**TODO: THIS IS BADDER**

## Model 7: P-value all-stars (squared)

Similar to my first solo model from the regression section, I select the coefficients which were significant in model 3.

```{r}
glm_model3_coef <- summary(glm_model3)$coefficients

glm_model7_significant_predictors <- glm_model3_coef %>%
  as_tibble %>%
  mutate(predictor = row.names(glm_model3_coef)) %>%
  rename(p = 4) %>%
  filter(p < .05)

model7_significant_predictors$predictor
```

I'll use these predictors with a parabolic basis function:

```{r}
df_glm_model7 <- df_raw %>%
  select(
    customer,
    region,
    xn_03,
    xn_07,
    xn_08,
    xa_05,
    xw_03,
    outcome
  )

glm_model7 <- train_glm(data = df_glm_model7)
```


## Model 8: basically random basis functions

In an effort to do something different from my regression models, I apply different basis functions to each group of sentiment predictors. My idea is to see if the use of various basis functions leads to better performance from one group of predictors.

```{r}
glm_model8 <- train_glm(
  formula = outcome ~ cos(xa_01) + cos(xa_02) + cos(xa_03) + cos(xa_04) + cos(xa_05) + cos(xa_06) + cos(xa_07) + cos(xa_08) +
    sin(xb_01) + sin(xb_02) + sin(xb_03) + sin(xb_04) + sin(xb_05) + sin(xb_06) + sin(xb_07) + sin(xb_08) +
    tan(xn_01) + tan(xn_02) + tan(xn_03) + tan(xn_04) + tan(xn_05) + tan(xn_06) + tan(xn_07) + tan(xn_08) + 
    tanh(xs_01) + tanh(xs_02) + tanh(xs_03) + tanh(xs_04) + tanh(xs_05) + tanh(xs_06) +
    poly(xw_01, 3) + poly(xw_02, 3) + poly(xw_03, 3),
  data = df_raw
)
```

## Model 9: Sentiment features

With an eye toward answering the question of if the sentiment features matter, I focus this model on those.

```{r}
df_glm_model9 <- df_raw %>%
  select(-region, -customer)

names(df_glm_model9)
```

Because we have to use different basis functions, I chose the cubic. Here I dynamically compose the formula:

```{r}
formula_terms <- paste("I(", names(df_glm_model9) %>% head(-1) ,"^3)", sep = "")
glm_model9_formula <- as.formula(paste("outcome ~", paste(formula_terms, collapse = "+")))
glm_model9_formula
```

Now we model:

```{r}
glm_model9 <- train_glm(
  formula = glm_model9_formula,
  data = df_glm_model9
)
```


# Comparison

```{r}
model_results <- resamples(
  list(
    fit_1 = glm_model1,
    fit_2 = glm_model2,
    fit_3 = glm_model3,
    fit_4 = glm_model4,
    fit_5 = glm_model5,
    fit_6 = glm_model6,
    fit_7 = glm_model7,
    fit_8 = glm_model8,
    fit_9 = glm_model9
  )
)
```

## Accuracy

```{r}
dotplot(model_results, "accuracy")
```

**NOTES:** 1 has strong accuracy and low kappa, suggesting that the model isn't guessing randomly?

## ROC

```{r}
dotplot(model_results, metric = "ROC")
```


