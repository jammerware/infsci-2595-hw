---
title: "Part iiC: Regression model predictions"
subtitle: "INFSCI 2595 Final"
author: "Ben Stein"
date: "4/27/2022"
output: html_document
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "knit") }
  )
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# About this document

In this document, I make predictions with two of my top-performing regression models in order to understand trends between the predicted log-response and the observed log-response across variables of interest.

# Setup

## Libraries and tools

```{r}
library(tidyverse)
source("./tools.R")
```

## Data

```{r}
df <- load_project_data(center_and_scale = TRUE) %>%
  select(-outcome_numeric)

df %>% glimpse()
```

## Models to evaluate

I chose model 5 as the best-performing model from iiA and model 6 because while it seems like it may be overfit, I'm curious about how its prediction and confidence trends relate to model 6.

```{r}
lm_model5 <- readr::read_rds("./models/lm_rank1_model5.rds")
lm_model6 <- readr::read_rds("./models/lm_rank3_model6.rds")
```

## Variables of interest

Based on performance in iiA, variables of interest are `region`, `customer`, and two top-performing sentiment variables (`xb_07` and `xn_08`).

# Analysis

Let's apply what we know about important variables to these models. Spoiler alert: we need a function that generates confidence and prediction intervals for a given model and visualization grid.

```{r}
generate_preds <- function(model, viz_grid) {
  preds <- predict(model, viz_grid, interval = "confidence") %>%
    as.data.frame %>%
    as_tibble %>%
    rename(
      pred = fit,
      ci_lwr = lwr,
      ci_upr = upr
    ) %>%
    bind_cols(
      predict(
        model,
        viz_grid,
        interval = "prediction"
      ) %>%
        as.data.frame %>%
        as_tibble %>%
        select(
          pred_lwr = lwr,
          pred_upr = upr
        )
    )
  
  viz_grid %>% bind_cols(preds)
}
```


## Model 5

Model 5 includes `customer` and both of the sentiment predictors. Let's explore combinations of these three.

### `customer` and `xb_07`

We need a tibble that varies the two variables we want to examine and holds the other variables constant. Let's make it.

```{r}
lm_model5_xb_07_vizgrid <- append(
  expand.grid(
    customer = df$customer %>% unique(),
    xb_07 = seq(
      min(df$xb_07),
      max(df$xb_07),
      length.out = 101
    )
  ),
  df %>%
    select(
      -customer,
      -region,
      -response_log,
      -xb_07
    ) %>%
    colMeans
) %>% as_tibble

lm_model5_xb_07_vizgrid
```

We also need predictions for our visualization grid:

```{r}
df_lm_model5_xb_07 <- generate_preds(lm_model5, lm_model5_xb_07_vizgrid)
```

We can finally visualize trends!

```{r}
df_lm_model5_xb_07 %>%
    ggplot(mapping = aes(x = xb_07)) + 
    ggtitle("xb_07 with respect to customer") +
    geom_ribbon(mapping = aes(ymin = pred_lwr, ymax = pred_upr), fill = "orange") +
    geom_ribbon(mapping = aes(ymin = ci_lwr, ymax = ci_upr), fill = "gray") +
    geom_line(mapping = aes(y = pred)) +
    facet_wrap(~ customer)
```

This, I suspect, is super important. For some customers (B, G, Other, K, and A), values of `xb_07` don't really affect predictive accuracy - the interval size is pretty flat. Note that this is not the same as saying that the predictive accuracy is equal for all these customers. Uncertainty is high for customer A and smaller for, say G or Other. But for some customers (say, D), extreme values of `xb_07` greatly influences predictive accuracy. It may make sense to conceptualize some customers as super-loyal. For these customers, sentiment in the report doesn't really matter, while for less-than-totally-loyal customers, extreme sentiment can cause problems. 

## Model 6

This model doesn't have an important categorical to examine, so let's look at just the sentiment predictors. We already took a close look at `xb_07`, so let's facet `xn_08` by `xb_07`.

### `xn_08` faceted by `xb_07`

```{r}
lm_model6_xb_07_xn_08_vizgrid <- append(
  expand.grid(
    xn_08 = seq(
      min(df$xn_08),
      max(df$xn_08),
      length.out = 101
    ),
    xb_07 = seq(
      min(df$xb_07),
      max(df$xb_07),
      length.out = 9
    ),
    # the most common level of these factors
    # borrowed from https://stackoverflow.com/questions/18433647/how-to-get-the-most-frequent-level-of-a-categorical-variable-in-r
    customer = which.max(table(df$customer)),
    region = which.max(table(df$region))
  ),
  df %>%
    select(
      -customer,
      -region,
      -response_log,
      -xb_07,
      -xn_08
    ) %>%
    colMeans
) %>% as_tibble

lm_model6_xb_07_xn_08_vizgrid
```

Make predictions and visualize trends:

```{r}
generate_preds(lm_model6, lm_model6_xb_07_xn_08_vizgrid) %>%
    ggplot(mapping = aes(x = xn_08)) + 
    ggtitle("xn_08 with respect to xb_07") +
    geom_ribbon(mapping = aes(ymin = pred_lwr, ymax = pred_upr), fill = "orange") +
    geom_ribbon(mapping = aes(ymin = ci_lwr, ymax = ci_upr), fill = "gray") +
    geom_line(mapping = aes(y = pred)) +
    facet_wrap(~ xb_07)
```

This is harder to interpret, but my read here is that at very high levels of `xb_07`, `xn_08` predicts with greater uncertainty. The bow-tie shape of all facets seems to suggest that extreme values of sentiment result in greater uncertainty. We can also see that uncertainty increases at the lower and higher cut points of `xb_07` (i.e. facets in the top and bottom rows have more uncertainty). So the moral of the story is... keep your customer relationships on a relatively even keel? 

FYI: this is my favorite skill I've learned in this class. Holding other variables constant in order to determine how one variable influences uncertainty is super cool. I'm confident that my interpretation is various shades of rudimentary and incorrect, but I still think this is a really interesting technique.