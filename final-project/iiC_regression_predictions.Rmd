---
title: "INFSCI 2595 Final"
subtitle: "Part IIc: Regression model predictions"
author: "Ben Stein"
date: "4/27/2022"
output: html_document
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "knit") }
  )
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# About this document

In this document, I make predictions with two of my top-performing regression models in order to understand trends between the predicted log-response and the observed log-response.

# Setup

## Libraries and tools

```{r}
library(tidyverse)
source("./tools.R")
```

## Data

```{r}
df <- load_project_data() %>%
  select(-outcome_numeric)

df %>% glimpse()
```

## Models to evaluate

I chose model 5 as the best-performing model from iiA and model 6 because while it seems like it may be overfit, I'm curious about how its prediction and confidence trends relate to model 6.

```{r}
lm_model5 <- readr::read_rds("./models/lm_rank1_model5.rds")
lm_model6 <- readr::read_rds("./models/lm_rank3_model6.rds")
```

# Analysis

A quick utility function to visualize the intervals/trends:

```{r}
get_pred_trends_df <- function(model, data) {
  # get the prediction/confidence intervals
  pred_data <- cbind(
    predict(model, newdata = data, interval = "confidence"),
    predict(model, newdata = data, interval = "prediction")
  ) %>%
    magrittr::set_colnames(c(
      "ci_fit",
      "ci_lwr",
      "ci_upr",
      "pred_fit",
      "pred_lwr",
      "pred_upr"
    )) %>%
    as_tibble 
  
  # combine with model dataset
  cbind(
    data,
    pred_data
  )
}
```

## Looking at `xb_07`

I decided to look at this sentiment predictor because it appeared as a solo predictor (or as an interaction) in all three of the top-performing linear models. 

**NOTE:** I suspect this is overspecific, because I didn't see any other common terms other than the categorical ones, and that seems wrong. So I'll look at this specific predictor, and then the groups of sentiment variables below, because, you know. Gotta do something.

### Model 5: Customer + `xb_07`

Model 5 has the `customer` categorical, so let's look at `xb_07` faceted by that.

```{r}
get_pred_trends_df(lm_model5, df_lm_model5) %>%
  ggplot(mapping = aes(x = xb_07)) +
  geom_ribbon(
    mapping = aes(
      ymin = pred_lwr,
      ymax = pred_upr
    ),
    fill = "orange"
  ) +
  geom_ribbon(
    mapping = aes(
      ymin = ci_lwr,
      ymax = ci_upr
    ),
    fill = "gray"
  ) +
  geom_line(mapping = aes(y = ci_fit)) + 
  facet_wrap(~ customer)
```

It seems notable that some `customer` groups have such large confidence intervals compared to others. At first I thought this might be due to imbalance in the customer factor's levels, but the relationship between numbers of observations and CI width is unreliable:

```{r}
df_lm_model5 %>%
  group_by(customer) %>%
  count %>%
  arrange(desc(n))
```

Notably, group A has the same number of observations as group B, but A's CI width is massive overall, and B's is much smaller. So I guess my argument here is that customers in some groups are much easier to predict than others, but I don't know why.

Also, I'm wondering if it's a problem that the bounds of the CI and the prediction interval are so close that I can't see them in the figures.

### Model 5: `region` + `xb_07`

```{r}
get_pred_trends_df(lm_model5, df_lm_model5) %>%
  ggplot(mapping = aes(x = xb_07)) +
  geom_ribbon(
    mapping = aes(
      ymin = pred_lwr,
      ymax = pred_upr
    ),
    fill = "orange"
  ) +
  geom_ribbon(
    mapping = aes(
      ymin = ci_lwr,
      ymax = ci_upr
    ),
    fill = "gray"
  ) +
  geom_line(mapping = aes(y = ci_fit)) + 
  facet_wrap(~ region)
```

At extreme values of `xb_07`, the intervals get noticeably wider. The exception is high values of in region `XX`. Region `ZZ` seems to have extra uncertainty.

### Model 6: `customer` + `xb_07`

```{r}
get_pred_trends_df(lm_model6, df_lm_model6) %>%
  ggplot(mapping = aes(x = xb_07)) +
  geom_ribbon(
    mapping = aes(
      ymin = pred_lwr,
      ymax = pred_upr
    ),
    fill = "orange"
  ) +
  geom_ribbon(
    mapping = aes(
      ymin = ci_lwr,
      ymax = ci_upr
    ),
    fill = "gray"
  ) +
  geom_line(mapping = aes(y = ci_fit)) + 
  facet_wrap(~ customer)
```

Unlike model 5, model 6 seems to have roughly equal uncertainty across values of `xb_07` and of `customer`. This may suggest it's a better model across the surface of predictors (?)

### Model 6: `region` + `xb_07`

```{r}
get_pred_trends_df(lm_model6, df_lm_model6) %>%
  ggplot(mapping = aes(x = xb_07)) +
  geom_ribbon(
    mapping = aes(
      ymin = pred_lwr,
      ymax = pred_upr
    ),
    fill = "orange"
  ) +
  geom_ribbon(
    mapping = aes(
      ymin = ci_lwr,
      ymax = ci_upr
    ),
    fill = "gray"
  ) +
  geom_line(mapping = aes(y = ci_fit)) + 
  facet_wrap(~ region)
```

Like model 5, there is more uncertainty at the outer values of `xb_07`. This seems especially true of high values in regions `XX` and `YY`.

### Summary of evaluating both models with respect to `xb_07`, customer, and region

In general, extreme values of the predictor increase uncertainty, particularly _high_ values as opposed to low ones. However, model 6 seems to have less variability in confidence/prediction interval than model 5 does.
