---
title: "INFSCI 2595 Final"
subtitle: "Part IIa: Regression models"
author: "Ben Stein"
date: "4/13/2022"
output: html_document
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "knit") }
  )
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# About this document

In this document, I fit nine regression models and evaluate them against the training data. In Part IId, I choose the best of these models for additional tuning and validation against a test set.

# Setup (libraries and tools)

## Libraries and tools

```{r}
library(tidyverse)
library(caret)
library(jtools)

source("./tools.R")
```

## Load (lightly) preprocessed data

Since this document is concerned with regression models, I select only the variables of interest.

```{r}
df <- load_project_data() %>%
  select(-outcome_numeric)

df %>% glimpse
```

# Models

For all regression models, I use 5-fold cross validation repeated 5 times using RMSE as the objective function. I center and scale all predictors.

```{r}
ctrl <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 5
)

metric_rmse <- "RMSE"

train_lm <- function(formula, data) {
  train(
    formula,
    data = data,
    method = "lm",
    preProcess = c("center", "scale"),
    metric = metric_rmse,
    trControl = ctrl
  )
}
```

## Model 1: categorical features only (linear additive)

```{r}
df_model1 <- df %>%
  select(where(is.factor), response_log)

df_model1 %>% names
```

```{r}
model1 <- train_lm(
  response_log ~ .,
  data = df_model1
)
```

## Model 2: Continuous features only (linear additive)

```{r}
df_model2 <- df %>%
  select(where(is.numeric))

df_model2 %>% names
```


```{r}
model2 <- train_lm(
  response_log ~ .,
  data = df_model2
)
```

## Model 3: All features (linear additive)

```{r}
df_model3 <- df
df_model3 %>% names
```

```{r}
model3 <- train_lm(
  response_log ~ .,
  data = df_model3
)
```

## Model 4: Interact region with continuous inputs

```{r}
df_model4 <- df %>%
  select(where(is.numeric), region)

df_model4 %>% names
```
 
```{r}
model4 <- train_lm(response_log ~ region * ., df_model4)
```

## Model 5: Interact customer with continuous inputs

```{r}
df_model5 <- df %>%
  select(where(is.numeric), customer)

df_model5 %>% names
```

**TODO:** problem?

```{r}
model5 <- train_lm(response_log ~ customer * ., df_model5)
```
 
 ## Model 6: All pairwise interactions of continuous inputs
 
```{r}
df_model6 <- df %>%
  select(is.numeric)

names(df_model6)
```
 
**TODO:** problem? 

```{r}
model6 <- train_lm(response_log ~ .^2, data = df_model6)
```
 
 ## Model 7: P-value all-stars (squared)
 
I'm throwing wild haymakers here (my signature move in this project), but for my first solo model, I decided to take the the predictors which were significant in the model with every variable included (Model 3, above) and apply a new basis function to continuous variable. I chose the parabolic as the new basis.
 
```{r}
model3_coef <- summary(model3)$coefficients

model7_significant_predictors <- model3_coef %>%
  as_tibble %>%
  mutate(predictor = row.names(model3_coef)) %>%
  rename(p = 4) %>%
  filter(p < .05)

model7_significant_predictors$predictor
```

Okay, we've got our predictors. Let's model.

```{r}
df_model7 <- df %>%
  select(
    region,
    customer,
    xa_02,
    xb_04,
    xb_07,
    xb_08,
    xn_04,
    xn_05,
    xn_08,
    xw_01,
    response_log
  )

df_model7
```

Now we can model with a parabolic basis function:

```{r}
model7 <- train_lm(
  formula = response_log ~ region +
    customer +
    I(xa_02^2) +
    I(xb_04^2) +
    I(xb_07^2) +
    I(xb_08^2) +
    I(xn_04^2) +
    I(xn_05^2) +
    I(xn_08^2) +
    I(xw_01^2),
  data = df_model7
)
```
 
 
## Model 8:
 
For this model, I chose predictors based on my EDA. I'm specifically interested in predictors which:

- Are sentiment-based and continuous
- Have an apparently Gaussian distribution
- Have a high number of unique values
- Are not highly correlated with other variables in the same group

Here are the variables that meet these criteria (and the response we're going to predict):

```{r}
df_model8 <- df %>% 
  select(
    xa_07,
    xa_08,
    xb_07,
    xb_08,
    xn_07,
    xn_08,
    xs_01,
    xs_02,
    xs_03,
    xs_04,
    response_log
  )
```

We need a new basis function to meet requirements. I'm going to try splines, because I thought some of the loess lines looked spliney. I chose a 20-degrees-of-freedom basis pretty much at random. Okay, let's model.

```{r}
model8 <- train_lm(
  formula = response_log ~ splines::ns(xa_07, df = 20) + 
    splines::ns(xa_08, df = 20) + 
    splines::ns(xb_08, df = 20) + 
    splines::ns(xb_08, df = 20) + 
    splines::ns(xn_08, df = 20) + 
    splines::ns(xn_08, df = 20) + 
    splines::ns(xs_01, df = 20) + 
    splines::ns(xs_02, df = 20) + 
    splines::ns(xs_03, df = 20) + 
    splines::ns(xs_04, df = 20),
  data = df_model8
)
```

 
## Model 9: `findCorrelation` based selection
 
Here, I use `findCorrelation` from `caret` to choose numeric predictors which have highest pairwise correlation. Note that this approach is different from model 8 because I don't consider apparent distribution or number of unique values. I also exclude categorical variables from this model.

```{r}
model9_correlated_predictors <- df %>%
  select(-region, -customer, -response_log) %>%
  cor %>%
  findCorrelation(cutoff = .8, names = TRUE)

model9_correlated_predictors
```
Including all non-correlated continuous predictors, I model and apply a low-degree `spline` basis function, because why not?

```{r}
df_model9 <- df %>%
  select(all_of(model9_correlated_predictors), response_log)
```

```{r}
model9 <- train_lm(
  formula = response_log ~ 
    splines::ns(xa_02, df = 3) +
    splines::ns(xa_04, df = 3) +
    splines::ns(xa_05, df = 3) +
    splines::ns(xb_01, df = 3) +
    splines::ns(xb_03, df = 3) +
    splines::ns(xb_04, df = 3) +
    splines::ns(xb_05, df = 3) +
    splines::ns(xb_06, df = 3) +
    splines::ns(xn_04, df = 3) +
    splines::ns(xn_05, df = 3),
  data = df_model9
)
```


# Comparison
 
```{r}
all_fits <- list(
  fit_1 = model1,
  fit_2 = model2,
  fit_3 = model3,
  fit_4 = model4,
  fit_5 = model5,
  fit_6 = model6,
  fit_7 = model7,
  fit_8 = model8,
  fit_9 = model9
)

model_results <- resamples(
  all_fits
)
```

## RMSE

```{r}
dotplot(model_results, df, metric='RMSE')
```

It's a little hard to see the RMSE because of the large variability in model 5. Let's look closer:

```{r}
all_fits[-5] %>%
  purrr::map(function(x) x$results$RMSE) %>%
  as_tibble
```

By RMSE, models 3 and 4 are strongest (in that order) with 7 and 2 not far behind.

## R^2

```{r}
dotplot(model_results, df, metric='Rsquared')
```
 
As with RMSE, models 3, 4, and 7 lead the pack. I'll save these models for comparison to later models.
 
```{r}
readr::write_rds(model3, "./models/lm_rank1_model3.rds")
readr::write_rds(model4, "./models/lm_rank2_model4.rds")
readr::write_rds(model7, "./models/lm_rank3_model7.rds")
```
 
 