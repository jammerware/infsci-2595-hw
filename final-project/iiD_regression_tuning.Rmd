---
title: "INFSCI 2595 Final"
subtitle: "Part iiD: Tuning and advanced regression models"
author: "Ben Stein"
date: "4/27/2022"
output: html_document
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "knit") }
  )
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# About this document

In this document, I train and tune advanced regression models. In the the next, I evaluate the performance of these models against each other by various metrics.

**NOTE:** In most cases, I run each model using `caret`'s default tuning grid and then re-run with a tuning grid based on those results. The untuned model is labeled with `_untuned`. 

# Setup (libraries and tools)

## Libraries and tools

```{r}
library(tidyverse)
library(caret)

source("./tools.R")
```

## Data

```{r}
df <- load_project_data() %>%
  select(-outcome_numeric)

df %>% glimpse
```

## Utility functions

For all models, I perform 5-fold cross validation repeated 5 times and use RMSE as the performance metric to target.

```{r}
supertrain_regression_info <- list(
  metric = "RMSE",
  ctrl = trainControl(
    method = "repeatedcv", 
    repeats = 5, 
    number = 5
  )
)
```

I'm also saving every model produced here for evaluation in the next document.

```{r}
save_model <- function(filename, model) {
  readr::write_rds(model, paste("./models/adv_reg_", filename ,".rds", sep = ""))
}
```

# Modeling

## Model 1: All categorical/continuous inputs (Linear additive)

```{r}
model1 <- my_supertrain(
  response_log ~ .,
  data = df,
  method = "lm",
  info = supertrain_regression_info
)

save_model("model1_lm_all", model1)
```

## Model 2: All pairwise interactions of continuous inputs, include additive categorical features

```{r}
model2 <- my_supertrain(
  response_log ~ region + customer + (. -region -customer)^2,
  data = df,
  method = "lm",
  info = supertrain_regression_info
)

save_model("model2_lm_continuous_interact", model2)
```

## Model 3: First model selected in iiA (model 5 in that document)

```{r}
model3_formerly5 <- my_supertrain(
  formula = response_log ~ customer * (.),
  data = df %>% select(-region),
  method = "lm",
  info = supertrain_regression_info
)

save_model("model3_lm_customer_interact_sentiment", model3_formerly5)
```

## Model 4: Second model selected in iiA (which was model 6 in that document)

```{r}
model4 <- my_supertrain(
  response_log ~ (.)^2, 
  data = df %>% select(where(is.numeric)),
  method = "lm",
  info = supertrain_regression_info
)

save_model("model4_lm_pairwise_sentiment", model4)
```

## Model 5: Regularized regression with Elastic net (All pairwise continuous, additive categorical features)

### Default tune

```{r}
model5_untuned <- my_supertrain(
  formula = response_log ~ region + customer + (. -region - customer ^2),
  data = df,
  method = "glmnet",
  info = supertrain_regression_info
)

model5_untuned
```

### Tuned

```{r}
model5 <- my_supertrain(
  formula = response_log ~ region + customer + (. -region - customer ^2),
  data = df,
  method = "glmnet",
  info = supertrain_regression_info,
  tuneGrid = expand.grid(
    alpha = seq(0.05, 1, by = .05),
    lambda = seq(
      min(model5_untuned$results$lambda),
      max(model5_untuned$results$lambda),
      length.out = 10
    )
  )
)

save_model("model5_lm_enet", model5)
```


## Model 6: Regularized regression with Elastic net (the more complex of the 2 models selected from iiA)

In iiA, I chose models 5 and 6 as the best performers. Model 6 includes pairwise interactions from all continuous inputs, so I deem it the more complex one.

```{r}
model6_untuned <- my_supertrain(
  formula = response_log ~ (. -region -customer)^2, 
  data = df,
  method = "glmnet",
  info = supertrain_regression_info
)

model6_untuned
```


```{r}
model6 <- my_supertrain(
  formula = response_log ~ (. -region -customer)^2, 
  data = df,
  method = "glmnet",
  info = supertrain_regression_info,
  tuneGrid = expand.grid(
    alpha = seq(0.05, 1, by = .05),
    lambda = seq(
      min(model6_untuned$results$lambda),
      max(model6_untuned$results$lambda),
      length.out = 10
    )
  )
)

save_model("model6_lm_enet_pairwise_sentiment", model6)
```


## Model 7: A neural net

I included all predictors here and did a simple one-layer neural net. First, I run the model with the default tune grid.

```{r}
model7_untuned <- my_supertrain(
  response_log ~ .,
  data = df,
  method = "nnet",
  info = supertrain_regression_info,
  trace = FALSE
)

model7_untuned
```

It looks like the default tuning grid stops at 5 units? Let's try more and play with tuning `decay`.

```{r}
model7 <- my_supertrain(
  response_log ~ .,
  data = df,
  method = "nnet",
  trace = FALSE,
  info = supertrain_regression_info,
  tuneGrid = expand.grid(
    size = 1:10,
    decay = seq(0, 1, length.out = 10)
  )
)

save_model("model7_nnet", model7)
```

## Model 8: Random forest

Once again, we start without a tuning grid:

```{r}
model8_untuned <- my_supertrain(
  response_log ~ .,
  data = df,
  method = "rf",
  info = supertrain_regression_info,
)

model8_untuned
```

`mtry` at 22 seems to cause the best metrics. Let's look around there a little bit.

```{r}
model8 <- my_supertrain(
  response_log ~ .,
  data = df,
  method = "rf",
  info = supertrain_regression_info,
  tuneGrid = expand.grid(
    mtry = 15:29
  )
)

save_model("model8_rf", model8)
```

## Model 9 : Gradient boosted tree (XGBoost)

Same deal as before: we'll run with the default tuning grid and then tune parameters.

```{r}
model9_untuned <- my_supertrain(
  response_log ~ .,
  data = df,
  method = "xgbTree",
  info = supertrain_regression_info
)

model9_untuned
```

```{r}
model9 <- my_supertrain(
  response_log ~ .,
  data = df,
  method = "xgbTree",
  info = supertrain_regression_info,
  tuneGrid = expand.grid(
    eta = seq(0.1, 0.6, by = 0.1),
    # gamma = c(0, 0.5, 0.9),
    gamma = 1,
    max_depth = 1:5,
    # min_child_weight = c(1, 2),
    min_child_weight = 1,
    colsample_bytree = seq(0.5, 0.8, by = 0.05),
    subsample = seq(0.5, 1, by = 0.1),
    nrounds = c(50, 100, 150)
  )
)

save_model("model9_xgb", model9)
```

## Model 10: Freestyle choice 1 - Support Vector Machines (SVM) with RBF

```{r}
model10_untuned <- my_supertrain(
  response_log ~ .,
  data = df,
  method = "svmRadial",
  info = supertrain_regression_info
)

model10_untuned
```

And tune:

```{r}
model10 <- my_supertrain(
  outcome ~ .,
  data = df,
  method = "svmRadial",
  info = supertrain_regression_info,
  tuneGrid = expand.grid(
    C = seq(0.1, 1, by = 0.1),
    sigma = seq(0.01, 0.1, length.out = 25)
  )
)

save_model("model10_svm_rbf", model10)
```

## Model 11: Freestyle choice 2 - KNN

First, a default-tuned version:

```{r}
model11_untuned <- my_supertrain(
  response_log ~ .,
  data = df,
  method = "knn",
  info = supertrain_regression_info
)

model11_untuned
```

The only parameter to tune is `k` (the number of neighbors whose values are considered in prediction), so let's tune it.

```{r}
model11 <- my_supertrain(
  response_log ~ .,
  data = df,
  method = "knn",
  info = supertrain_regression_info,
  tuneGrid = expand.grid(
    k = 3:15
  )
)

save_model("model11_knn", model11)
```

# Conclusion

In [the next document](./iiD_regression_tuning_discussion.html), we'll load up all these models and compare them to find the best ones.
