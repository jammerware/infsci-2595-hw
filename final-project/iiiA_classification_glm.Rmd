---
title: "INFSCI 2595 Final"
subtitle: "Part iiiA: Simple classification models (GLM)"
author: "Ben Stein"
date: "04/27/2022"
output: html_document
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "knit") }
  )
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# About this document

In this document, I fit simple `glm` classification models to predict the `outcome` variable.

# Setup

## Libraries & tools

```{r}
library(tidyverse)
library(caret)

source("./tools.R")
```

## Data

```{r}
df <- load_project_data(outcome_to_numeric = FALSE) %>%
  select(-response_log)

df %>% glimpse
```

# Modeling

A quick utility function so I don't have to write `family = "binomial"` 9 times.

**NOTE:** This part of the assignment asks us to use `glm` rather than tuning the model with `caret`. I couldn't figure out how to compute ROC, sensitivity, and specificity over multiple thresholds easily without caret, so I just used it and only did 5-fold cv to minimize resampling.

```{r}
train_glm <- function(formula = outcome ~ ., data) {
  set.seed(2022)
  
  train(
    formula,
    data = data,
    method = "glm",
    family = "binomial",
    metric = "ROC",
    preProcess = c("center", "scale"),
    trControl = trainControl(
      method = "cv", 
      number = 5,
      classProbs = TRUE,
      savePredictions = TRUE,
      summaryFunction = twoClassSummary
    )
  )
}
```

## Model 1: Categorical variables (linear additive)

```{r}
glm_model1 <- train_glm(
  formula = outcome ~ .,
  data = df %>% select(where(is.factor))
)
```

## Model 2: Continuous variables (linear additive)

```{r}
glm_model2 <- train_glm(
  data = df %>% select(where(is.numeric), outcome)
)
```

## Model 3: All variables (linear additive)

```{r}
glm_model3 <- train_glm(data = df)
```

## Model 4: Interact region with continuous inputs

```{r}
glm_model4 <- train_glm(
  formula = outcome ~ region * .,
  data = df %>% select(
    where(is.numeric),
    region,
    outcome
  )
)
```

## Model 5: Interact customer with continuous inputs

```{r}
glm_model5 <- train_glm(
  formula = outcome ~ customer * .,
  data = df %>% 
    select(
      where(is.numeric),
      customer,
      outcome
    )
)
```

## Model 6: All pairwise interactions of continuous inputs

```{r}
glm_model6 <- train_glm(
  formula = outcome ~ .^2,
  data = df %>% select(where(is.numeric), outcome)
)
```

## Model 7: P-value all-stars (squared)

Similar to my first solo model from the regression section, I select the coefficients which were significant in model 3.

```{r}
glm_model3_coef <- summary(glm_model3)$coefficients

glm_model7_significant_predictors <- glm_model3_coef %>%
  as_tibble %>%
  mutate(predictor = row.names(glm_model3_coef)) %>%
  rename(p = 4) %>%
  filter(p < .05)

glm_model7_significant_predictors$predictor
```

I'll use these predictors with a parabolic basis function:

```{r}
glm_model7 <- train_glm(
  formula = outcome ~
    customer +
    region +
    I(xn_03^2) +
    I(xn_07^2) +
    I(xn_08^2) +
    I(xa_05^2) +
    I(xw_03^2),
  data = df
)
```

## Model 8: basically random basis functions

In an effort to do something different from my regression models, I apply different basis functions to each group of sentiment predictors. My idea is to see if the use of various basis functions leads to better performance from one group of predictors.

```{r}
glm_model8 <- train_glm(
  formula = outcome ~ cos(xa_01) + cos(xa_02) + cos(xa_03) + cos(xa_04) + cos(xa_05) + cos(xa_06) + cos(xa_07) + cos(xa_08) +
    sin(xb_01) + sin(xb_02) + sin(xb_03) + sin(xb_04) + sin(xb_05) + sin(xb_06) + sin(xb_07) + sin(xb_08) +
    tan(xn_01) + tan(xn_02) + tan(xn_03) + tan(xn_04) + tan(xn_05) + tan(xn_06) + tan(xn_07) + tan(xn_08) + 
    tanh(xs_01) + tanh(xs_02) + tanh(xs_03) + tanh(xs_04) + tanh(xs_05) + tanh(xs_06) +
    poly(xw_01, 3) + poly(xw_02, 3) + poly(xw_03, 3),
  data = df
)
```

## Model 9: Sentiment features

With an eye toward answering the question of if the sentiment features matter, I focus this model on those.

```{r}
df_glm_model9 <- df %>%
  select(-region, -customer)

names(df_glm_model9)
```

Because we have to use different basis functions, I chose the cubic. Here I dynamically compose the formula:

```{r}
formula_terms <- paste("I(", names(df_glm_model9) %>% head(-1) ,"^3)", sep = "")
glm_model9_formula <- as.formula(paste("outcome ~", paste(formula_terms, collapse = "+")))
glm_model9_formula
```

Now we model:

```{r}
glm_model9 <- train_glm(
  formula = glm_model9_formula,
  data = df_glm_model9
)
```

# Comparison

I'll examine a couple of statistics to help me decide which of these seems best. 

```{r}
all_glm_models <- list(
  model1 = glm_model1,
  model2 = glm_model2,
  model3 = glm_model3,
  model4 = glm_model4,
  model5 = glm_model5,
  model6 = glm_model6,
  model7 = glm_model7,
  model8 = glm_model8,
  model9 = glm_model9
)

glm_results <- resamples(all_glm_models)
```

## Accuracy

```{r}
dotplot(glm_results)
```

## ROC, Sensitivty, and Specificity

```{r}
dotplot(model_results)
```

Models 2, 3, and 9 have the best combination of these stats. Let's how AIC stacks up:

## AIC

```{r}
tibble::tibble(
  model = names(all_glm_models),
  aic = purrr::map_dbl(all_glm_models, function(x) AIC(x$finalModel))
) %>%
  ggplot(mapping = aes(x = model, y = aic)) + 
  geom_col()
```

Our leading models all have relatively equivalent AICs, so my guess is that it's dealer's choice between these three. Saving them for later:

```{r}
save_model("simple_cls_model2", glm_model2)
save_model("simple_cls_model3", glm_model3)
save_model("simple_cls_model9", glm_model9)
```

