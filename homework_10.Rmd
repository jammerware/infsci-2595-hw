---
title: "INFSCI 2595 Spring 2022 Homework: 10"
subtitle: "Assigned April 7, 2022; Due: April 14, 2022"
author: "Ben Stein"
date: "Submission time: April 14, 2022 at 11:00PM EST"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Collaborators

I completed this assignment on my own. 

## Overview

This assignment focuses on the architecture of single hidden layer feedforward neural networks. You will practice calculating hidden units and neural network responses for a regression task. You will gain experience understanding the interaction between the hidden unit and output layer parameters. You will fit a regression neural network to data by minimizing the sum of squared errors (SSE), and tune the number of hidden units via a hold-out test set. Lastly, you will use `caret` to manage the resampling and tuning of a neural network for you.  

**IMPORTANT**: code chunks are created for you. Each code chunk has `eval=FALSE` set in the chunk options. You **MUST** change it to be `eval=TRUE` in order for the code chunks to be evaluated when rendering the document.  

You are allowed to add as many code chunks as you see fit to answer the questions.  

## Load packages

This assignment will use packages from the `tidyverse` suite.  

```{r, load_packages}
library(tidyverse)
```

This assignment also uses the `scale_color_colorblind()` function from the `ggthemes` package. If you do not have `ggthemes` already installed please type `install.packages("ggthemes")` into the `R` console, or use the RStudio package installer GUI. You only need to run that command **ONCE**. Once the package is installed, you do **not** need to run the command again.  

## Problem 01

In lecture we discussed that neural networks are just matrix multiplications. Hidden units are essentially transformed linear models. The output layer is a linear basis function model with the hidden units acting as the basis functions. In this problem, you will work through the various matrix calculations. Neural networks have already been fit to a data set and the the neural network parameters (weights and biases) are provided to you.  

As practice, you will work with a noise-free toy problem. The response, $f$, is simply equal to $\mathrm{sin}\left(x\right)$. You will practice neural network calculations to approximate that functional relationship.  

The toy data set is loaded for you in the code chunk below and a glimpse is printed to screen. To distinguish between noisy observations (that we typically work with) the response is named `f` in the data set. The input is named `x` as in previous homework assignments.  

```{r, read_prob_01_data}
url_01 <- 'https://raw.githubusercontent.com/jyurko/INFSCI_2595_Spring_2022/main/HW/10/hw_10_prob_01_train.csv'
prob_01_df <- readr::read_csv(url_01, col_names = TRUE)
prob_01_df %>% glimpse()
```

### 1a)

**Plot `f` with respect to `x` with `ggplot()`. Since the response is noise-free, use `geom_line()` and `geom_point()` geoms to make "connect the dots".**  

#### SOLUTION

```{r, solution_01a}
prob_01_df %>%
  ggplot(mapping = aes(x = x, y = f)) +
  geom_point() + 
  geom_line()
```

### 1b)

A set of neural network parameters are downloaded in the code chunk below. The parameters are stored in a list consisting of two elements. The first element named, `beta_matrix` is a matrix containing the $\beta$-parameters associated with the hidden units. The second, `alpha_vector`, is a "regular vector" containing the output layer parameters. The list is printed to screen for you.  

```{r, read_prob_01_params_a}
url_load_dir <- 'https://github.com/jyurko/INFSCI_2595_Spring_2022/blob/main/HW/10'
url_load_01_2a <- paste(paste(url_load_dir, "hw_10_prob_01_params_2a.rds", sep = "/"), 
                        "raw=true",
                        sep="?")

params_01_2a <- readr::read_rds( file = url_load_01_2a )
params_01_2a
```

The `$beta_matrix` is constructed such that each column corresponds to a separate *hidden unit* within the hidden layer. The particular neural network model associated with the `params_01_2a` parameters therefore has 2 hidden units.  

**Why does the `$beta_matrix` contain 2 rows and why does `$alpha_vector` consist of 3 elements? What does each $\beta$ parameter correspond to relative to the hidden units?**  

#### SOLUTION

The two rows in `beta_matrix` correspond to the two unknowns to be learned (the slope and intercept, or weight and bias). Its two columns correspond to the two hidden units. The output layer, which is modeled with `alpha_vector`, has three elements because it has a single intercept (bias) and a weight/slope for each hidden unit's output.

### 1c)

The hidden units consist of two calculations. The first calculates the "linear predictors" based on the inputs and hidden unit parameters. The second is a non-linear transformation of the "linear predictors". We discussed in lecture that there are many possible non-linear transformation functions to use, but the logistic function is a popular choice. Assume the inputs are stored in a design matrix $\mathbf{X}$, which includes a column of 1s, and the hidden unit parameters are stored in a matrix $\mathbf{B}$.  

**Write out the expressions for the linear predictor matrix $\mathbf{A}$ and the non-linear hidden unit values $\mathbf{H}$ assuming a logistic (inverse logit) function is used.**  

#### SOLUTION

##### Linear predictors

$$
x_{n,:}B_k
$$

##### Non-linear transformation of the predictors (logistic function)

$$
g(x) = \frac{e^x}{1-e^x}
$$

##### Hidden layer overall output

$$
h_{n,k}(x_{n,:}) = g(x_{n,:} \beta_k)
$$


### 1d)

You will now define a function which calculates the hidden unit "linear predictor" and non-linear values. The function is named `calc_hidden_units()` and it has three input arguments. The first, `X`, is the input design matrix, the second, `B`, is the hidden unit parameter matrix, and the third is the non-linear transformation function. The transformation function argument is named `g` to be consistent with the lecture notation of $g\left( \cdot \right)$.  

This function is general and therefore allows passing in an arbitrary non-linear transformation function.  

**Complete the code chunk below. Calculate the linear predictor matrix `A` and the non-linear transformed hidden unit matrix `H`. The results are returned in a list for you.**  

#### SOLUTION

```{r, solution_01d}
calc_hidden_units <- function(X, B, g)
{
  ### your code
  A <- X %*% B
  H <- g(A)
  
  ### book keeping
  return(list(A = A, H = H))
}
```

### 1e)

You will calculate the hidden units for the `prob_01_df` dataset and the `params_01_2a` parameters. The code chunk below provides a function which visualizes the hidden units with respect to a single input, `x`. You will use this function to interpret the behavior of the hidden units. `viz_hidden_trend_wrt_x()` accepts three input arguments. The first, `v_mat`, is a matrix of hidden unit values. The second, `x_df`, is a `data.frame` which must contain a column named `x`. The third, `trend_type`, is a character string containing the "type" of hidden unit values contained in the `v_mat` matrix. The `trend_type` variable is assigned to the legend title. The `trend_type` argument is used to state whether the resulting figure is plotting the hidden unit "linear predictors" or the non-linear hidden unit values.  


```{r, define_hidden_viz_func}
viz_hidden_trend_wrt_x <- function(v_mat, x_df, trend_type)
{
  x_df %>% 
    select(all_of(c("x"))) %>% 
    tibble::rowid_to_column("obs_id") %>% 
    left_join(v_mat %>% as.data.frame() %>% 
                tibble::as_tibble() %>% 
                purrr::set_names(sprintf("h_%02d", 1:ncol(v_mat))) %>% 
                tibble::rowid_to_column("obs_id"),
              by = "obs_id") %>% 
    pivot_longer(!c("obs_id", "x")) %>% 
    ggplot(mapping = aes(x = x, y = value)) +
    geom_vline(xintercept = 0, color = 'grey50') +
    geom_line(mapping = aes(color = name, group = name),
              size = 1.15) +
    ggthemes::scale_color_colorblind(trend_type) +
    theme_bw() +
    theme(legend.position = "top")
}
```


**IMPORTANT**: In this problem you will use the **logistic** function as the non-linear (activation) function.  

**Complete the two code chunks below. You must create the design matrix `X01`. You must call the `calc_hidden_units()` function by passing in the appropriate arguments and storing the result to `nnet_hidden_2a`. Two calls to `viz_hidden_trend()` are started for you. Complete the calls by assigning the correct element from `nnet_hidden_2a`, `A` or `H`, as the first argument to the `viz_hidden_trend()` call.**  

*HINT*: The third argument to `viz_hidden_trend_wrt_x()` tells you to whether to assign the "linear predictors" or the non-linear hidden unit values.  

*HINT*: Use the `$` operator to access the elements from the list `nnet_hidden_2a`.  

*HINT*: What function can we use for the logistic function?  

#### SOLUTION

```{r, solution_01e_a}
X01 <- model.matrix(f ~ x, data = prob_01_df)

nnet_hidden_2a <- calc_hidden_units(X01, params_01_2a$beta_matrix, boot::inv.logit)
```

Visualize the hidden unit trends below.  

```{r, solution_01e_b}
viz_hidden_trend_wrt_x(
  nnet_hidden_2a$A,
  prob_01_df,
  "hidden linear predictors")

viz_hidden_trend_wrt_x(
  nnet_hidden_2a$H,
  prob_01_df,
  "hidden non-linear units")
```

### 1f)

**Describe the trends of the hidden linear predictors relative to the $\beta$ parameter values displayed in Problem 1b).**  

#### SOLUTION

The trends of the linear predictors here align exactly with the beta parameters from 1b because the beta parameters represent the linear model within each neuron. For example, the second hidden predictor has a negative slope because $\beta_1$ from 1b is negative.

### 1g)

The neural network response, $f$, is calculated as a linear combination of the non-linear hidden unit values in the output layer. Assume the output layer consists of an intercept (bias) $\alpha_{0}$ and a column vector of slopes (weights) $\boldsymbol{\alpha}$.  

**Write out the expression for the response vector $\mathbf{f}$ given the output layer parameters, $\alpha_{0}$, $\boldsymbol{\alpha}$, and the matrix of non-linear hidden unit values $\mathbf{H}$.**  

#### SOLUTION

$$
f = \alpha_0 + \mathbf{H} \boldsymbol{\alpha}
$$

### 1h)

As with the hidden units, you will use a function to calculate the neural network response. The code chunk below defines the `calc_nnet_response()` function. It accepts two arguments. The first, `H`, is the matrix of non-linear hidden unit values and the second, `a`, is the "regular vector" of output layer parameters. Note that `a` contains the intercept (the bias) and the slopes (the weights).  

**Complete the code chunk below. You must separate the `a` vector into the intercept (bias) and the slopes (weights). Store the intercept as the `a_0` variable and the slopes as the `a_w` regular vector. You must then convert the `a_w` regular vector into a column vector `a_col`. Finally, you must calculate the response, `f`. The response is returned as a vector for you.**  

#### SOLUTION

```{r, solution_01h}
calc_nnet_response <- function(H, a)
{
  ### separate the vector into bias and weights
  a0 <- a[1]
  a_w <- a[-1]
  
  # convert the weights to a column vector
  # it seems like transposing this actually breaks it for some reason?
  a_col <- a_w
  
  # calculate the response (the output layer)
  # browser()
  f <- a0 + H %*% a_col
  
  as.vector(f)
}
```

### 1i)

With all the calculations completed, let's now compare the neural network response to the true sine wave.  

**Complete the code chunk below by assigning the correct the arguments to the `calc_nnet_response()` function. The rest of the code, which generates the figure, is completed for you.**  

**How would you describe the neural network's fit? Which portions are approximated well?**  

#### SOLUTION

##### Plot

```{r, solution_01i}
# 
prob_01_df %>% 
  mutate(nnet_f = calc_nnet_response(nnet_hidden_2a$H, params_01_2a$alpha_vector)) %>% 
  ggplot(mapping = aes(x = x)) +
  geom_line(mapping = aes(y = nnet_f),
            color = "black", size = 1.15) +
  geom_point(mapping = aes(y = f),
             color = "red", shape = 1, size = 3.5) +
  labs(y = "f") +
  theme_bw()
```

##### About the fit

The fit seems decent, but is best around low absolute values of `x`. 

## Problem 02

You will now repeat your calculations from Problem 1. You will first try out a different set of parameters for a two hidden unit neural network. Then you consider a neural network with 5 hidden units.  

### 2a)

The code chunk below reads in a new set of neural network parameters. The format is consistent with that from Problem 1, in that the hidden unit parameters are contained within the element `$beta_matrix` and the output layer parameters are stored in `$alpha_vector`. The parameters are printed to the screen below.  

```{r, read_in_prob_02a_params_b}
url_load_01_2b <- paste(paste(url_load_dir, "hw_10_prob_01_params_2b.rds", sep = "/"), 
                        "raw=true",
                        sep="?")

params_01_2b <- readr::read_rds( url_load_01_2b )
params_01_2b
```

You will use these new parameters to calculate the hidden unit "linear predictors" and the non-linear hidden unit values for the two hidden unit neural network.  

**Complete the code chunk below. Call the `calc_hidden_units()` function with the appropriate arguments and assign the result to the `nnet_hidden_2b` object. Then assign the correct arguments to the `viz_hidden_trend()` functions calls. Are the trends of the hidden unit values (linear and non-linear) consistent with the trends from Problem 1? If not, what would be causing the change? Based on the figures generated in the code chunk below, do you think the resulting neural network will be different from that in Problem 1?**  

#### SOLUTION

##### Plots

```{r, solution_02}
nnet_hidden_2b <- calc_hidden_units(X01, params_01_2b$beta_matrix, boot::inv.logit)

viz_hidden_trend_wrt_x( 
  nnet_hidden_2b$A, 
  prob_01_df,
  "hidden linear predictors")
  
viz_hidden_trend_wrt_x(
  nnet_hidden_2b$H, 
  prob_01_df,
  "hidden non-linear units")
```

##### Are the trends of the hidden unit values (linear and non-linear) consistent with the trends from Problem 1? If not, what would be causing the change?

They're similar, but not the same (for example, the first hidden unit has a negative slope in this problem compared to a positive one in problem 1). The differences are caused by different beta slopes/intercepts provided by the parameter object.

##### Based on the figures generated in the code chunk below, do you think the resulting neural network will be different from that in Problem 1?**  

Yes. Because the hidden layer responds to the input differently, the network itself is different.

### 2b)

Let's now compare the responses associated with the two sets of parameters.  

**Complete the first code chunk below by assigning the correct arguments to the two `calc_nnet_response()` calls. The first call is associated with the parameters from Problem 1, with the result stored to the `nnet_fa` variable. The second call is associated with the new set of parameters, and the result is stored to the `nnet_fb` variable.**  

**The second code chunk is completed for you. It plots the two responses together with the true sine wave output. Based on the figure below, how do the two different neural network models compare?**  

#### SOLUTION

```{r, solution_02b_a}
results_2hidden <- prob_01_df %>% 
  mutate(
    nnet_fa = calc_nnet_response(nnet_hidden_2a$H, params_01_2a$alpha_vector),
    nnet_fb = calc_nnet_response(nnet_hidden_2b$H, params_01_2b$alpha_vector)
  )
```

Now visualize the neural network predictions and compare to the true sine wave.  

```{r, solution_02b_b}
results_2hidden %>% 
  pivot_longer(!c("x", "f")) %>% 
  ggplot(mapping = aes(x = x)) +
  geom_line(mapping = aes(y = value,
                          group = name,
                          linetype = name,
                          color = name),
            size = 1.15) +
  geom_point(mapping = aes(y = f),
             color = "red", size = 3, shape = 1) +
  ggthemes::scale_color_colorblind("Model") +
  scale_linetype_discrete("Model") +
  labs(y = "f") +
  theme_bw()
```

##### Based on the figure below, how do the two different neural network models compare?

The networks appear similar or identical.

### 2c)

**Based on your results, can you "explain" or interpret the neural network behavior by ONLY examining the slopes (weights) acting on the inputs?**  

#### SOLUTION

Clearly not - despite having different slopes/weights, the resulting neural networks respond similarly/identically.

### 2d)

Let's now perform the same type of calculations, but on a neural network with 5 hidden units instead of 2. As before, you will compare two sets of parameters for the 5 hidden unit model. The code chunk below reads in the two different sets of neural network parameters. The format is consistent with that from Problem 1, except now there are more parameters since there are more hidden units.  


```{r, read_in_prob_2_5_hidden_params}
url_load_01_5a <- paste(paste(url_load_dir, "hw_10_prob_01_params_5a.rds", sep = "/"), 
                        "raw=true",
                        sep="?")

params_02_5a <- readr::read_rds( url_load_01_5a )

url_load_01_5b <- paste(paste(url_load_dir, "hw_10_prob_01_params_5b.rds", sep = "/"), 
                        "raw=true",
                        sep="?")

params_02_5b <- readr::read_rds( url_load_01_5b )
```

The first set neural network parameters, `params_02_5a`, are displayed for you below to show the difference in structure with the 2 hidden unit neural networks you worked with previously.  

```{r, show_prob_2_5_hidden_params_a}
params_02_5a
```


**Calculate the hidden unit "linear predictors" and non-linear values for the two different 5 hidden unit models. Then complete the calls to the `viz_hidden_trend_wrt_x()` function to plot the hidden unit trends with respect to the input.**  

**Describe the trends of the non-linear values for the fifth hidden unit, `h_05`, relative to its "linear predictor" values. Discuss the differences between the two models (sets of parameters).**  

#### SOLUTION

Calculate the hidden unit "linear predictors" and non-linear hidden unit values.  

```{r, solution_02d_a}
nnet_hidden_5a <- calc_hidden_units(X01 , params_02_5a$beta_matrix, boot::inv.logit)
nnet_hidden_5b <- calc_hidden_units(X01, params_02_5b$beta_matrix, boot::inv.logit)
```

Visualize the behavior of the hidden units associated with the `params_02_5a` parameters.  

```{r, solution_02d_b}
viz_hidden_trend_wrt_x(
  nnet_hidden_5a$A, 
  prob_01_df,
  "hidden linear predictors")

viz_hidden_trend_wrt_x(
  nnet_hidden_5a$H, 
  prob_01_df,
  "hidden non-linear units")
```

Visualize the behavior of the hidden units associated with the `params_02_5b` parameters.  

```{r, solution_02d_c}
viz_hidden_trend_wrt_x(
  nnet_hidden_5b$A, 
  prob_01_df,
  "hidden linear predictors")

viz_hidden_trend_wrt_x(
  nnet_hidden_5b$H, 
  prob_01_df,
  "hidden non-linear units")
```

##### Describe the trends of the non-linear values for the fifth hidden unit, `h_05`, relative to its "linear predictor" values. 

The linear predictor for `h_05` is, as expected a simple line with negative slope. However, the non-linear value has a sigmoid shape, coercing low absolute values of x to values of "y" around the intercept.

##### Discuss the differences between the two models (sets of parameters).

In general, the `a` set of parameters produce higher nonlinear output values from the linear predictors. 

### 2e)

Let's now calculate the neural network response for both with 5 hidden units.  

**Complete the first code chunk below by assigning the arguments correctly to the two `calc_nnet_response()` function calls. The first call is intended for the model associated with `params_02_5a` while the second call is intended for the model associated with `params_02_5b`.**  

**The second code chunk is completed for you. How do the two models compare to each other and to the true sine wave?**  

#### SOLUTION

```{r, solution_02e_a}
results_5hidden <- prob_01_df %>% 
  mutate(
    nnet_fa = calc_nnet_response(nnet_hidden_5a$H, params_02_5a$alpha_vector),
    nnet_fb = calc_nnet_response(nnet_hidden_5b$H, params_02_5b$alpha_vector)
  )
```

Visualize the two neural network model predictions and compare with the true sine wave.  

```{r, solution_02e_b, eval=FALSE}
results_5hidden %>% 
  pivot_longer(!c("x", "f")) %>% 
  ggplot(mapping = aes(x = x)) +
  geom_line(mapping = aes(y = value,
                          group = name,
                          linetype = name,
                          color = name),
            size = 1.15) +
  geom_point(mapping = aes(y = f),
             color = "red", size = 3, shape = 1) +
  ggthemes::scale_color_colorblind("Model") +
  scale_linetype_discrete("Model") +
  labs(y = "f") +
  theme_bw()
```

### 2f)

The code chunk below is completed for you. The 2 hidden unit and 5 hidden unit model predictions are compared side by side.  

```{r, solution_02f_a, eval=FALSE}
results_2hidden %>% 
  mutate(num_hidden = 2) %>% 
  bind_rows(results_5hidden %>% 
              mutate(num_hidden = 5)) %>% 
  pivot_longer(!c("x", "f", "num_hidden")) %>% 
  tidyr::separate(name,
                  c("nnet_word", "fparams"),
                  sep = "_") %>% 
  tidyr::separate(fparams,
                  c("fltr", "paramset"),
                  sep = 1) %>% 
  ggplot(mapping = aes(x = x)) +
  geom_line(mapping = aes(y = value,
                          group = interaction(paramset,
                                              num_hidden),
                          linetype = paramset,
                          color = paramset),
            size = 1.15) +
  geom_point(mapping = aes(y = f),
             color = "red", size = 3, shape = 1) +
  facet_grid( ~ num_hidden, labeller = "label_both") +
  ggthemes::scale_color_colorblind("Params") +
  scale_linetype_discrete("Params") +
  labs(y = "f") +
  theme_bw()
```

**Based on the figure above, which model, the 2 hidden units or 5 hidden units, performs better? What controls complexity within a neural network model and how could you go about "tuning" that complexity?**  

#### SOLUTION

The 5 unit model performs better based on this input. Complexity within the network is controlled by the number of neurons in each hidden layer and the number of layers. More neurons and layers result in higher complexity (and increased chance to overfit).

### 2g)

The toy data within Problem 1 and 2 comes from a simple sine wave:  

$$ 
f\left(x\right) = \mathrm{sin}\left(x\right)
$$

Let's see if a linear model, using the correct `sin()` basis can correctly identify that the "slope" acting on `sin(x)` is 1.  

**Use the `lm()` function to a fit a linear model for the response `f` and the sine of the input, `sin(x)`. Use the `prob_01_df` data set as the `data` argument to the `lm()` call. Assign the result to the `lm_sine_mod` object.**  

**Print the `summary()` of the `lm()` call to the screen. What is the estimate for the slope associated with the `sin(x)`?**  

#### SOLUTION

```{r, solution_02g}
lm_sine_mod <- lm(f ~ sin(x), data = prob_01_df)

### print the summary to the screen
summary(lm_sine_mod)
```

##### What is the estimate to the slope acting on `sin(x)`?  

The estimated slope is 1.

### 2h)

**How many unknown parameters were there in the linear model with the sine wave basis function? How many unknown parameters existed in the neural network with 5 hidden units?**  

#### SOLUTION

What do you think?  

### 2i)

**It was really simple to fit the linear model. Why would we want to use a neural network when we can build the exact model in this application using `lm()`?**  

#### SOLUTION

My guess here is that we only were able to fit a linear model with `sin` because the data was synthetic, and we know the function we used to generate it. If we didn't it'd be difficult to guess the right basis function, while a neural net would learn the basis function naturally.  

## Problem 03

In the previous problems, you focused on the predictions of a neural network. You will now work through fitting neural networks, on a slightly more realistic example. The code chunk below reads a data set consisting of 3 continuous inputs, `x1`, `x2`, and `x3`, and a continuous response, `y`. A glimpse of the data set is displayed to the screen for you.  

```{r, read_prob_03_data}
url_03_train <- 'https://raw.githubusercontent.com/jyurko/INFSCI_2595_Spring_2022/main/HW/10/hw_10_prob_03_train.csv'

prob_03_df <- readr::read_csv( url_03_train, col_names = TRUE)
prob_03_df %>% glimpse()
```

### 3a)

The wide-format data set is converted into a long-format data set for you in the code chunk below. The glimpse displayed to the screen shows that the inputs have been "stacked" or "gathered" together into a column `name` with their values given in the `value` column.  

```{r, make_03_longformat}
prob_03_lf <- prob_03_df %>% 
  tibble::rowid_to_column("obs_id") %>% 
  pivot_longer(!c("obs_id", "y"))

prob_03_lf %>% glimpse()
```

**Plot the noisy response, `y`, with respect to each input using the long-format data set. Using the `geom_point()` geom and create separate facets (subplots) for each input using `facet_wrap()`.**  

**What trends do you see in the scatter plots?**  

#### SOLUTION

#### Plots

```{r, solution_03a}
prob_03_lf %>%
  ggplot(mapping = aes(y = y)) +
  geom_point(mapping = aes(x = value)) + 
  facet_wrap(~ name)
```

##### Trends

`x1` does not have a clear relationship to `y`. `x2` may have a very weak positive linear relationship with `y`, while `x3` may have a similarly weak negative linear relationship to `y`.

### 3b)

In the previous problems you used a logistic function as the non-linear function associated with each hidden unit. However, there are many different functions that could be used. To get exposure working with a different "activation" function, you will use the hyperbolic tangent function for this problem. Let's first get an idea about how the hyperbolic tangent compares with the logistic function.  

**Complete the code chunk below by setting `x` to be 101 equally spaced points between -5.5 and 5.5. Calculate the the logistic function of `x` and assign the result to `logistic_result`. Calculate the hyperbolic tangent of `x` and assign the result to `tanh_result`. The result of the code chunk is completed for you. It visualizes the non-linear transformations with respect to the `x` variable.**  

**Is the hyperbolic tangent function similiar to the logistic function? In what ways are the two different?**  

*HINT*: The hyperbolic tangent function in `R` is `tanh()`.  

```{r, solution_03b}
tibble::tibble(
  x = seq(-5.5, 5.5, length.out = 101)
) %>% 
  mutate(
    logistic_result = boot::inv.logit(x),
    tanh_result = tanh(x)) %>% 
  # rest of the code here is completed for you
  pivot_longer(!c("x")) %>% 
  ggplot(mapping = aes(x = x, y = value)) + 
  geom_hline(yintercept = c(-1, 0, 1),
             color = 'grey50', linetype = 'dashed') +
  geom_line(mapping = aes(y = value,
                          color = name,
                          linetype = name),
            size = 1.15) +
  ggthemes::scale_color_calc("") +
  scale_linetype_discrete("") +
  theme_bw() +
  theme(legend.position = "top") 
```

##### Is the hyperbolic tangent function similiar to the logistic function? In what ways are the two different?

The hyperbolic tangent function is similar to the logistic function in shape, but unlike the logistic function, values of the `tanh` function may be negative (ranging from -1 to 1 rather than 0 to 1).

### 3c)

You will fit the neural network by minimizing the sum of squared errors (SSE). Thus, we will work with a non-probabilistic setting, even though we derived the linear and generalized linear model fitting with likelihoods and priors. Remember that minimizing the SSE is analogous to maximizing a Gaussian log-likelihood!  

**Write the expression for the SSE using the observed response $y$ and the neural network response $f$. You may write the SSE in either the summation or matrix/vector notation. If you use a summation notation use the subscript $n$ to denote a single observation. If you use matrix/vector notation denote the response vector as $\mathbf{y}$.**

#### SOLUTION

$$
argmin_{\beta_k=0, ... \beta_{k = H}, \alpha, \alpha_0} \left( \sum_{n=1}^N \left(  \left( y_n - f_n \right)^2 \right) \right)
$$

### 3d)

You will now program the error function we wish to minimize in the style of the log-posterior functions from earlier homework assignments. You will name your function `my_neuralnet_sse()`. It will consist of two input arguments, a vector of parameters to learn and a list of required information. Before defining the function, you will create the list of required information, which is started for you in the code chunk below. Notice that the structure is similar to the lists of information created for the generalized linear models. However, two pieces of information not associated with GLMs are required for the neural network. The variable `$num_hidden` specifies the number of hidden units and the variable `$transform_hidden` stores the non-linear transformation function to apply to each hidden unit.  

You will start out with a small neural network consisting of 3 hidden units. You will use the hyperbolic tangent as the non-linear transformation function, instead of the logistic function that you worked with in the previous problems. You will therefore need to assign the `tanh()` function correctly to the `$transform_hidden` field in the list. Be careful about the `()` when assigning the function *object*!  

You will need to specify the design matrix for your neural network based on the 3 inputs in the `prob_03_df` data set. Think carefully how the design matrix is structured in a neural network.  

**Complete the list of required information by completing the code chunk below. You must create the design matrix based on the three inputs. Assign the design matrix to the `$design_matrix` variable in the list. Assign the observed responses to the `$yobs` variable in the list. Set the number of hidden units to be 3.**  

**After specifying the `info_three_units` list, calculate the total number of parameters in the single hidden layer neural network with 3 hidden units and assign the result to the `info_three_units$num_params` variable.**  

#### SOLUTION

The code chunk is started for you below.  

```{r, solution_03d}
### design matrix
Xmat_03 <- model.matrix(y ~ ., data = prob_03_df)

info_three_units <- list(
  yobs = prob_03_df$y,
  design_matrix = Xmat_03,
  num_hidden = 3,
  transform_hidden = tanh
)

# number of hidden units times (the number of inputs + 1 for the intercept)
info_three_units$num_params <- info_three_units$num_hidden * ncol(Xmat_03)
```

The total number of hidden units you calculated in the above code chunk are printed to the screen below.  

```{r, solution_03d_b}
info_three_units$num_params
```


### 3e)

You will now define the $SSE$ objective in the `my_neuralnet_sse()` function below. As described previously, the function consists of two input arguments. The first argument, `theta`, contains all of the unknown parameters to learn. The vector is organized with all hidden unit parameters listed before the output layer parameters. The first part of the `my_neuralnet_sse()` function has several portions completed for you. **You are responsible for determining the number of hidden unit parameters (the betas) for each hidden unit.** You should **not** hard code `length_beta_per_unit`. You must then calculate the total number of hidden unit parameters and assign the result to `total_num_betas`. Again you should **not** hard code this number because later on you will try out more hidden units.  

The hidden unit parameters are extracted from the `theta` vector and organized into the `Bmat` matrix with dimensions consistent with the $\mathbf{B}$ described in Problem 01 and 02.  

The output layer parameters are extracted for you and assigned to the `a_all` vector. You must reorganize the output layer parameters by separating the bias, `a0`, and output layer weights, `aw`. The bias should be a scalar quantity and the output layer weights should be a "regular vector".  

You must complete the function by performing the necessary matrix math calculations, transformations, and calculation of the $SSE$. The comments in the function describe what you must complete in each line.  

After completing the function, test that it works using two separate guesses for the unknown parameters. First set all parameters to a value of 0, then set all parameters to a value of -1.25. If your function is specified correctly the $SSE$ should be `683.113` for the guess of all 0's and it should be `2238.39` for the guess -1.25 for all parameters.  

**Complete the `my_neuralnet_sse()` function below and test it's operation with the two guesses specified in the problem statement.**  

#### SOLUTION

The `my_neuralnet_sse()` function is started for you in the code chunk below.  

```{r, solution_03e}
my_neuralnet_sse <- function(theta, my_info)
{
  # extract the hidden unit parameters
  X <- my_info$design_matrix
  length_beta_per_unit <- ncol(X)
  total_num_betas <- my_info$num_hidden * length_beta_per_unit
  beta_vec <- theta[1:total_num_betas]
  
  # reorganize the beta parameters into a matrix
  Bmat <- matrix(beta_vec, nrow = length_beta_per_unit, byrow = FALSE)
  
  # extract the output layer parameters
  a_all <- theta[(total_num_betas + 1):length(theta)]
  
  # reorganize the output layer parameters by extracting
  # the output layer intercept (the bias)
  a0 <- a_all[1]
  aw <- a_all[-1]
  
  # calculate the linear predictors associated with
  # each hidden unit
  A <- X %*% Bmat
  
  # pass through the non-linear transformation function
  H <- my_info$transform_hidden(A)
  
  # calculate the response (the output layer)
  f <- a0 + H %*% matrix(aw)
  
  # calculate the SSE
  (f - my_info$yobs)^2 %>% sum
}
```

Test out your `my_neuralnet_sse()` function with values of 0 for all parameters.  

**NOTE:** I added a quick function to calculate the number of unknowns.

```{r}
calc_num_unknowns <- function(nn_info) {
  nn_info$num_hidden * ncol(nn_info$design_matrix) + nn_info$num_hidden + 1
}
```


```{r, solution_03e_b}
my_neuralnet_sse(
  rep(0, calc_num_unknowns(info_three_units)), 
  info_three_units
)
```

Test out your `my_neuralnet_sse()` function with values of -1.25 for all parameters.  

```{r, solution_03e_c}
my_neuralnet_sse(
  rep(-1.25, calc_num_unknowns(info_three_units)),
  info_three_units
)
```

### 3f)

With the objective function completed, it's now time to fit the simple neural network with 3 hidden units. You will use the `optim()` function to perform the optimization, just as in the previous assignments. Since we are focused on finding the estimates at the moment, you will work with `optim()` itself, rather than within the `my_laplace()` wrapper as in previous assignments.  

You will fit two neural networks from two different starting guess values. The first starting guess will be a vector of 0's, and the second guess will be -1.25 for all parameters. Complete the two code chunks below by specifying the initial guesses correctly and completing the remaining input arguments to the `optim()` call. You must set the `gr` argument to `r NULL` so that `optim()` uses finite differences to estimate the gradient vector. Pass in the `info_three_units` list of required information to both `optim()` calls. Specify the `method` argument to be `"BFGS"` to use the quasi-Newton BFGS algorithm. Set the `hessian` argument to be `r FALSE` which forces the Hessian matrix to **NOT** be estimated at the end. We are simply interested in the point estimates at the moment and so we will not be concerned with the curvature of the error surface. The maximum number of iterations is set for you in both `optim()` calls already.  

**Complete both code chunks below in order to fit the three hidden unit neural network with two different starting guesses. Follow the instructions in the problem statement to specify all the arguments to the `optim()` calls.**  

**After fitting, print out the identified optimal parameters contained in the `$par` field of the `optim()` results for both cases. Are the identified optimal parameter values the same between the two starting guesses? Why would the results not be the same?**  

#### SOLUTION

Fit the neural network with the initial guess of 0's for all parameters.  

```{r, solution_03f_a}
optim_fit_3_a <- optim( 
  rep(0, calc_num_unknowns(info_three_units)),
  my_neuralnet_sse,
  gr = NULL,
  my_info = info_three_units,
  method = "BFGS",
  hessian = FALSE,
  control = list(maxit = 5001))
```


Fit the neural network with the initial guess of -1.25 for all parameters.  

```{r, solution_03f_b}
optim_fit_3_b <- optim( 
  rep(-1.25, calc_num_unknowns(info_three_units)),
  my_neuralnet_sse,
  gr = NULL,
  my_info = info_three_units,
  method = "BFGS",
  hessian = FALSE,
  control = list(maxit = 5001))
```


Compare the optimized parameter estimates.  

##### Starting from 0s

```{r, solution_03f_c}
optim_fit_3_a$par
```

##### Starting from -1.25s

```{r}
optim_fit_3_b$par
```

The optimized parameter values are not the same. This is because the surface is not unimodal as it was with the linear models we studied. Each of the above represents a local maximum that may or may not be the actual global max.

### 3g)

Fit the neural network with 3 hidden units again, but this time use 2 randomly generated initial guess values. Use standard normals (mean 0 and standard deviation 1) to generate the initial guesses.  

**Complete the two code chunks below by generating two random initial guess values. Assign the first random initial guess to `init_guess_03_c` and the second random initial guess to `init_guess_03_d`.**  
**Complete the `optim()` calls following the same instructions as the previous question.**  

**Check if the optimized parameter estimates are the same or not.**  

#### SOLUTION

Set the random initial guess values.  

```{r, solution_03g_a}
num_unknowns <- calc_num_unknowns(info_three_units)

set.seed(412412)
init_guess_03_c <- rnorm(num_unknowns)
  
set.seed(214214)
init_guess_03_d <- rnorm(num_unknowns)
```

Run the optimization for the first random initial guess.  

```{r, solution_03g_b}
optim_fit_3_c <- optim( 
  init_guess_03_c,
  my_neuralnet_sse,
  gr = NULL,
  my_info = info_three_units,
  method = "BFGS",
  hessian = FALSE,
  control = list(maxit = 5001))
```

Run the optimization for the first random initial guess.  

```{r, solution_03g_c}
optim_fit_3_d <- optim(
  init_guess_03_d,
  my_neuralnet_sse,
  gr = NULL,
  my_info = info_three_units,
  method = "BFGS",
  hessian = FALSE,
  control = list(maxit = 5001))
```

Compare the parameter estimates.  

##### Random guess 1

```{r, solution_03g_d}
optim_fit_3_c$par
```

##### Random guess 2

```{r}
optim_fit_3_d$par
```

These parameter estimates are not the same, suggesting that they arrived at different maxima.

### 3h)

The `optim()` results store the objective function value as the `$value` field in the returned list object.  

**Compare the SSE for the 4 different starting guesses. Which model is better, as viewed by the training set?**  

#### SOLUTION

```{r}
df_3h <- list(
  guess = paste("g", seq(1:4), sep = ""),
  sse = c(
    optim_fit_3_a$value,
    optim_fit_3_b$value,
    optim_fit_3_c$value,
    optim_fit_3_d$value
  )
)

df_3h %>%
  as.tibble %>%
  ggplot(mapping = aes(x = as.factor(guess), y = sse)) + 
  geom_col()
```  

The second through fourth guesses look best. Let's look closer:

```{r}
all.equal(
  optim_fit_3_b$value,
  optim_fit_3_c$value,
  optim_fit_3_d$value,
)
```

They're all exactly equal, so every guess except the first is the best.

## Problem 04

You now have the major pieces in place for fitting neural networks! In this problem, we will fit additional neural networks with more hidden units!  

### 4a)

Let's define a function which will generate a random initial guess for the appropriate number of unknown parameters and then execute the `optim()` call. The `train_1layer_nnet_sse()` function has 4 input arguments. The first argument, `num_hidden`, is the number of hidden units in the hidden layer, the second, `transform_func`, is the non-linear transformation (activation) function, the third `X`, is the design matrix, and the fourth, `y`, the response vector.  

**Complete the code chunk below which assembles the list of required information and generates the random initial guess, for an arbitrary number of hidden units in the first hidden layer. Do not set the random seed inside the `train_1layer_nnet_sse()` function. We will set the seed before we fit the models.**  

*HINT*: If your function below is setup correctly you should be able to replicate the previous results if the **SAME** random seed is used. The second code chunk below resets the random seed for you as a confirmation test.  

#### SOLUTION

```{r, solution_04a}
train_1layer_nnet_sse <- function(num_hidden, transform_func, X, y)
{
  my_info_list <- list(
    yobs = y,
    design_matrix = X,
    num_hidden = num_hidden,
    transform_hidden = transform_func
  )
  
  # my_info_list$num_params <- # total number of hidden and output layer parameters
  my_info_list$num_params <- num_hidden * ncol(X) + num_hidden + 1
  
  # generate random initial guess
  init_guess <- rnorm(my_info_list$num_params)
  
  # call optim to fit the neural network
  optim( 
    init_guess,
    my_neuralnet_sse,
    gr = NULL,
    my_info = my_info_list,
    method = "BFGS",
    hessian = FALSE,
    control = list(maxit = 10001))
}
```

As a check fit the 3 hidden unit neural network again with the same random seed as used with `init_guess_03_c`. You should get the same parameters as `optim_fit_3_c`.  

```{r, solution_04a_b}
set.seed(412412)
check_optim_fit_3_c <- train_1layer_nnet_sse(3, tanh, Xmat_03, prob_03_df$y)
```

Compare to the previous `optim_fit_3_c` results.  

```{r, solution_04a_c}
optim_fit_3_c$par
```

```{r}
check_optim_fit_3_c$par
```

They're equal, so I'm good to go.

### 4b)

Let's now fit neural networks with 6, 12, and 24 hidden units instead of 3 hidden units. You will use two different initial guesses for each hidden layer size. The random seeds are set for you to make sure the results are reproducible.  

**Complete the three code chunks below by setting the input arguments to fit 2 pairs of 6, 12, and 24 hidden unit neural networks.**  

#### SOLUTION

```{r, solution_04b}
set.seed(412412)
optim_fit_6_a <- train_1layer_nnet_sse(6, tanh, Xmat_03, prob_03_df$y)

set.seed(214214)
optim_fit_6_b <- train_1layer_nnet_sse(6, tanh, Xmat_03, prob_03_df$y)
```

```{r, solution_04b_b}
set.seed(412412)
optim_fit_12_a <- train_1layer_nnet_sse(12, tanh, Xmat_03, prob_03_df$y)

set.seed(214214)
optim_fit_12_b <- train_1layer_nnet_sse(12, tanh, Xmat_03, prob_03_df$y)
```

Please note that fitting the two 24 hidden unit neural networks may take a few minutes.  

```{r, solution_04b_c}
set.seed(412412)
optim_fit_24_a <- train_1layer_nnet_sse(24, tanh, Xmat_03, prob_03_df$y)

set.seed(214214)
optim_fit_24_b <- train_1layer_nnet_sse(24, tanh, Xmat_03, prob_03_df$y)
```


### 4c)

Compare the training set SSE across all of the models you trained with random initial guesses (including the 3 hidden unit models).  

**Which model was considered the best according to the training set?**  

#### SOLUTION

```{r}
models <- list(
  optim_fit_3_c,
  optim_fit_3_d,
  optim_fit_6_a,
  optim_fit_6_b,
  optim_fit_12_a,
  optim_fit_12_b,
  optim_fit_24_a,
  optim_fit_24_b
)

sses <- models %>%
  purrr::map_dbl(my)

df_results <- list(
  model = c("3c", "3d", "6a", "6b", "12a", "12b", "24a", "24b"),
  sse = models %>% map_dbl(function(x) x$value)
) %>% as.tibble

df_results
```

On the training set, model 24a has the best performance by SSE.

## Problem 05

We know that we should **not** compare models strictly based on the training set (unless we were using an information criterion metric). The code chunk below reads in a hold-out test set for you. This hold out test set will be used to compare the performance across the hidden layer sizes that you have fit so far.  

```{r, read_holdout_set}
url_03_test <- 'https://raw.githubusercontent.com/jyurko/INFSCI_2595_Spring_2022/main/HW/10/hw_10_prob_03_test.csv'
prob_03_test_df <- readr::read_csv( url_03_test, col_names = TRUE)
prob_03_test_df %>% glimpse()
```

### 5a)

You will define a function which makes predictions for you and calculates the Mean Squared Error (MSE) on the test set. The function, `assess_nnet_mse()`, is started for you in the code chunk below. The first argument, `theta`, is a vector of all unknown parameters, the second argument, `num_hidden`, is the number of hidden unit parameters, the third argument, `transform_func`, is the non-linear transformation function, the fourth argument, `X`, is a design matrix, and the fifth argument, `y`, is the response vector.  

**You have worked with the necessary pieces to complete this function several different ways in this assignment. You are free to decide how best to calculate the MSE for a given set of parameters. The only requirement is that `assess_nnet_mse()` should return a scalar number.**  

#### SOLUTION

```{r, solution_05a}
assess_nnet_mse <- function(theta, num_hidden, transform_func, X, y)
{
  # extract the hidden unit parameters
  length_beta_per_unit <- ncol(X)
  total_num_betas <- num_hidden * length_beta_per_unit
  beta_vec <- theta[1:total_num_betas]
  
  # reorganize the beta parameters into a matrix
  Bmat <- matrix(beta_vec, nrow = length_beta_per_unit, byrow = FALSE)
  
  # extract the output layer parameters
  a_all <- theta[(total_num_betas + 1):length(theta)]
  
  # reorganize the output layer parameters by extracting
  # the output layer intercept (the bias)
  a0 <- a_all[1]
  aw <- a_all[-1]
  
  # calculate the linear predictors associated with
  # each hidden unit
  A <- X %*% Bmat
  
  # pass through the non-linear transformation function
  H <- transform_func(A)
  
  # calculate the response (the output layer)
  f <- a0 + H %*% matrix(aw)
  
  # calculate the SSE
  (f - y)^2 %>% sum
}
```

### 5b)

Before you can calculate the MSE on the hold-out test set, you must create the test design matrix.  

**Create the appropriate test design matrix associated with all 3 inputs, using the `prob_03_test_df` data set, and assign the result to `Xtest_03`.**  

#### SOLUTION

```{r, solution_05b}
Xtest_03 <- model.matrix(y ~ ., data = prob_03_test_df)
```

### 5c)

**Calculate the MSE for each of the models trained with random initial guess values. You are free to decide how to execute this task.**  

#### SOLUTION

##### Utility function to make assessment easier

```{r}
calc_mse_5 <- function(params, num_hidden) {
  assess_nnet_mse(
    theta = params,
    num_hidden = num_hidden,
    transform_func = tanh,
    X = Xtest_03,
    y = prob_03_test_df$y
  )
}
```

##### 3-unit models

```{r}
models_5 <- list(
  model_name = c("3c", "3d", "6a", "6b", "12a", "12b", "24a", "24b"),
  params = models %>% map(function(x) x$par),
  num_hidden = c(3, 3, 6, 6, 12, 12, 24, 24)
) %>% 
  as.tibble # %>% 
  # mutate(sse = calc_mse_5(params, num_hidden))

models_5
```


```{r}
calc_mse_5(optim_fit_3_c$par, 3)
```

```{r}
calc_mse_5(optim_fit_3_d$par, 3)
```

```{r}
calc_mse_5(optim_fit_6_a$par, 6)
```

```{r}
calc_mse_5(optim_fit_6_b$par, 6)
```

```{r}
calc_mse_5(optim_fit_12_a$par, 12)
```

```{r}
calc_mse_5(optim_fit_12_b$par, 12)
```
 
```{r}
calc_mse_5(optim_fit_24_a$par, 24)
```

```{r}
calc_mse_5(optim_fit_24_b$par, 24)
```
 

### 5d)

**Which model is the best as viewed by the hold-out test set performance?**  

#### SOLUTION

Based on test set performance, the 6-unit models were best (with the first random guess being barely better than the second.)

## Problem 06

You not only fit neural networks from scratch, but you used a hold-out test set to **tune** the number of hidden units! Doing so required you to directly work with the assumptions of the neural network, learning how to make predictions with the matrix operations, calculate the performance metric, and ultimately assess the potential for overfitting as the complexity increases. Understanding the assumptions and concepts are critical when assessing the behavior and performance of a neural network in a practical application when we use existing functions and packages to fit the neural network for us. In this last problem you will practice using `caret` to manage the training, evaluation, and tuning of a neural network. You will use the `nnet` package to fit the neural network. Please download and install `nnet` if you do not have it already. If you do not install it, `caret` will prompt you to install it and so please check the R console if nothing seems to happen when you use the `caret::train()` function.  

The code chunk below loads the `caret` package for you. You do not need to load `nnet`, the `caret` package will manage that for you.  

```{r, load_caret_package}
library(caret)
```

### 6a)

You must specify the resampling scheme that `caret` will use to train, assess, and tune the model.  

**Specify the resampling scheme to be 5 fold with 3 repeats. Assign the result of the `trainControl()` function to the `my_ctrl` object. Specify the primary performance metric to be `'RMSE'` and assign that to the `my_metric` object.**  

#### SOLUTION

```{r, solution_06a}
my_ctrl <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 3,
)

my_metric <- "RMSE"
```

### 6b)

In a realistic application, it is always best to first fit linear models before we fit neural networks. The linear models (especially regularized models which include interaction features) serve as interpretable baseline models. However, since this assignment is focused on neural networks we will just fit the neural network. You must train, assess, and tune a neural network using the **default** `caret` tuning grid. In the `caret::train()` function you must use the formula interface to specify the inputs are `x1`, `x2`, and `x3`, while the response is `y`. Assign the `method` argument to `'nnet'` and set the `metric` argument to `my_metric`. You must also instruct `caret` to standardize the features by setting the `preProcess` argument equal to `c('center', 'scale')`. Assign the `trControl` argument to the `my_ctrl` object.  

**Train, assess, and tune the `nnet` neural network with the defined resampling scheme. Assign the result to the `nnet_default` object and print the result to the screen. Which tuning parameter combinations are considered to be the best?**  

**IMPORTANT**: include the argument `trace = FALSE` in the `caret::train()` function call. This will make sure the `nnet` package does NOT print the optimization iteration results to the screen.  

#### SOLUTION

##### Model fitting

```{r, solution_06b}
set.seed(412412)
nnet_default <- train(
  y ~ .,
  data = prob_03_df,
  method = "nnet",
  metic = my_metric,
  trControl = my_ctrl,
  preProcess = c("center", "scale"),
  trace = FALSE
)

nnet_default
```

The optimal parameters were **model size 5** and **decay = 0.1**.

##### Assessment

```{r}
calc_mse_5(nnet_default$finalModel$wts, 5)
```

This seems **wrong**, but I tried.

### 6c)

You will only use the default `caret` tuning grid in this assignment. We could customize it to see if the performance could be improved, but for now the default grid is all we will use.  

**What do the two tuning parameters in the `nnet` package correspond to?**  

#### SOLUTION

According to the documentation, the `size` parameter corresponds to the number of units in the single hidden layer, while decay is a regularization parameter that prevents overfitting.

# Dumping ground

This is extra code I want to save for later but isn't used for the assignment.


```{r, eval=FALSE}
env_global <- as.environment(".GlobalEnv")
env_vars <- env_global %>% ls

fit_results_names <- env_vars[grepl("^optim_fit*", env_vars)]

df_results <- as.tibble(
  list(model = fit_results_names)
) %>%
  mutate(expression = paste(model, "$value", sep = "")) %>%
  add_column(sse= eval(quote(expression)))
# mutate(sse = eval(parse(text=expression)))

# optimization_values <- purrr::map()
df_results
```